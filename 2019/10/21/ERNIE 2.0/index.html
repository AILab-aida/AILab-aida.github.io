<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=7.4.0">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg?v=7.4.0" color="#222">
  <link rel="alternate" href="/atom.xml" title="AILab-aida" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: true,
    lazyload: false,
    pangu: true,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="ERNIE 2.0 发布了，刷新了 SOTA，而且中文上做了不少优化，这种大杀器作为一个 NLP 工程师我觉得有必要深入了解了解，最好能想办法用到工作中。 ERNIE 2.0 是基于持续学习的语义理解预训练框架，使用多任务学习增量式构建预训练任务。ERNIE 2.0 中，新构建的预训练任务类型可以无缝的加入训练框架，持续的进行语义理解学习。通过新增的实体预测、句子因果关系判断、文章句子结构重建等语">
<meta name="keywords" content="算法">
<meta property="og:type" content="article">
<meta property="og:title" content="ERNIE 2.0">
<meta property="og:url" content="https://ailab-aida.github.io/2019/10/21/ERNIE 2.0/index.html">
<meta property="og:site_name" content="AILab-aida">
<meta property="og:description" content="ERNIE 2.0 发布了，刷新了 SOTA，而且中文上做了不少优化，这种大杀器作为一个 NLP 工程师我觉得有必要深入了解了解，最好能想办法用到工作中。 ERNIE 2.0 是基于持续学习的语义理解预训练框架，使用多任务学习增量式构建预训练任务。ERNIE 2.0 中，新构建的预训练任务类型可以无缝的加入训练框架，持续的进行语义理解学习。通过新增的实体预测、句子因果关系判断、文章句子结构重建等语">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://qnimg.lovevivian.cn/github-nlp-baidu-ernie-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/github-nlp-baidu-ernie-2.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/github-nlp-baidu-ernie-3.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/github-baidu-nlp-ernie-3.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/github-baidu-nlp-ernie-4.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/github-baidu-nlp-ernie-5.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/github-baidu-nlp-ernie-6.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/github-baidu-nlp-ernie-7.jpeg">
<meta property="og:updated_time" content="2019-10-21T03:05:45.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ERNIE 2.0">
<meta name="twitter:description" content="ERNIE 2.0 发布了，刷新了 SOTA，而且中文上做了不少优化，这种大杀器作为一个 NLP 工程师我觉得有必要深入了解了解，最好能想办法用到工作中。 ERNIE 2.0 是基于持续学习的语义理解预训练框架，使用多任务学习增量式构建预训练任务。ERNIE 2.0 中，新构建的预训练任务类型可以无缝的加入训练框架，持续的进行语义理解学习。通过新增的实体预测、句子因果关系判断、文章句子结构重建等语">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/github-nlp-baidu-ernie-1.jpeg">
  <link rel="canonical" href="https://ailab-aida.github.io/2019/10/21/ERNIE 2.0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>ERNIE 2.0 | AILab-aida</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AILab-aida</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">一个专注技术的组织</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-sitemap">
      
    

    <a href="/atom.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/qq1074123922" class="github-corner" title="AILab-aida GitHub" aria-label="AILab-aida GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://ailab-aida.github.io/2019/10/21/ERNIE 2.0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AILab-aida">
      <meta itemprop="description" content="涉猎的主要编程语言为 深度学习、机器学习、大数据、服务端、移动端、前端、爬虫(go、scala、Java、flutter、Python、react、Vue)等。">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AILab-aida">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">ERNIE 2.0

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-10-21 11:01:13 / 修改时间：11:05:45" itemprop="dateCreated datePublished" datetime="2019-10-21T11:01:13+08:00">2019-10-21</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>ERNIE 2.0 发布了，刷新了 SOTA，而且中文上做了不少优化，这种大杀器作为一个 NLP 工程师我觉得有必要深入了解了解，最好能想办法用到工作中。</p><blockquote>
<p><strong>ERNIE 2.0</strong> 是基于持续学习的语义理解预训练框架，使用多任务学习增量式构建预训练任务。<strong>ERNIE 2.0</strong> 中，新构建的预训练任务类型可以无缝的加入训练框架，持续的进行语义理解学习。通过新增的实体预测、句子因果关系判断、文章句子结构重建等语义任务，<strong>ERNIE 2.0</strong> 语义理解预训练模型从训练数据中获取了词法、句法、语义等多个维度的自然语言信息，极大地增强了通用语义表示能力。</p>
</blockquote><a id="more"></a>

<p>这是官方的描述，我觉得那张配图可能更加直观：</p>
<p><img src="http://qnimg.lovevivian.cn/github-nlp-baidu-ernie-1.jpeg" alt></p>
<p>老习惯还是先理论再实践，所以看 paper 先，我们只看关键信息。</p>
<h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a><a href="#论文" title="论文"></a>论文</h2><p>ERNIE 核心思想：更多任务的 Bert。</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><a href="#Abstract" title="Abstract"></a>Abstract</h3><p>ERNIE 2.0 通过多任务学习获取语料中的更多信息，包括：词、句共现，词法，句法，语义等。</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><a href="#Introduction" title="Introduction"></a>Introduction</h3><p>出发点依然是摘要中提到的，举了几个例子挺不错：</p>
<ul>
<li>命名实体可能包含一些观念上的信息（不应拆分或作为普通词处理）</li>
<li>句子顺序和邻近句子能让模型学到结构（句法）表示</li>
<li>文档语义相似度或句子间的话语关系能让模型学到语义表示</li>
</ul>
<p>ERNIE 使用多任务学习，它们共享 encoding 网络（即词法、句法和语义编码信息可以在每项任务中使用），而且很容易定制训练任务（上图右下的 Pre-training Task 可以随意调整）。</p>
<h4 id="Unsupervised-Transfer-Learning-for-Language-Representation"><a href="#Unsupervised-Transfer-Learning-for-Language-Representation" class="headerlink" title="Unsupervised Transfer Learning for Language Representation"></a><a href="#Unsupervised-Transfer-Learning-for-Language-Representation" title="Unsupervised Transfer Learning for Language Representation"></a>Unsupervised Transfer Learning for Language Representation</h4><ul>
<li>context-independent<ul>
<li>Word2Vec, GloVe: 通过词共现</li>
</ul>
</li>
<li>context-dependent<ul>
<li>ELMo: Language model</li>
<li>OpenAI GPT: Transformer</li>
<li>BERT: Mask language model with a next sentence prediction task</li>
<li>XLM: Two methods to learn cross-lingual language model</li>
<li>MT-DNN: Learning several supervised tasks in GLUE based on pre-trained model</li>
<li>XLNet: Transformer-XL, learns bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order.</li>
</ul>
</li>
</ul>
<h4 id="Continual-Learning"><a href="#Continual-Learning" class="headerlink" title="Continual Learning"></a><a href="#Continual-Learning" title="Continual Learning"></a>Continual Learning</h4><p>在多个任务序列上训练模型，模型在学习新任务时能记起之前学过的任务。灵感来源于人类的学习过程。</p>
<h3 id="The-ERNIE-2-0-Framework"><a href="#The-ERNIE-2-0-Framework" class="headerlink" title="The ERNIE 2.0 Framework"></a><a href="#The-ERNIE-2-0-Framework" title="The ERNIE 2.0 Framework"></a>The ERNIE 2.0 Framework</h3><p>ERNIE 2.0 可以通过不断引入各种各样的预训练任务帮助模型高效学习词、句法和语义的表征。通过多任务学习不断更新预训练模型。在微调时，首先用预训练模型的参数初始化，然后根据特定任务的数据进行微调。</p>
<h4 id="Continual-Pre-training"><a href="#Continual-Pre-training" class="headerlink" title="Continual Pre-training"></a><a href="#Continual-Pre-training" title="Continual Pre-training"></a>Continual Pre-training</h4><p>包含两个步骤：</p>
<ul>
<li>使用大规模数据和先验知识持续构建无监督预训练任务<ul>
<li>构建不同类型的任务：单词感知任务、结构感知任务、语义感知任务</li>
<li>所有任务均为 self-supervised 或 weak-supervised，无需人工标注</li>
</ul>
</li>
<li>通过多任务逐步更新 ERNIE 模型<ul>
<li>首先 train 一个简单的初始模型</li>
<li>每增加一个任务，使用上一个模型的参数初始化，新模型用先前的模型训练</li>
</ul>
</li>
</ul>
<p>架构如下：</p>
<p><img src="http://qnimg.lovevivian.cn/github-nlp-baidu-ernie-2.jpeg" alt></p>
<ul>
<li>共享的 encoder</li>
<li>两种 loss functions</li>
</ul>
<h4 id="Fine-tuning-for-Application-Tasks"><a href="#Fine-tuning-for-Application-Tasks" class="headerlink" title="Fine-tuning for Application Tasks"></a><a href="#Fine-tuning-for-Application-Tasks" title="Fine-tuning for Application Tasks"></a>Fine-tuning for Application Tasks</h4><p>可以适应各种自然语言理解任务，比如：问答、推理、语义相似度等。</p>
<h3 id="ERNIE-2-0-Model"><a href="#ERNIE-2-0-Model" class="headerlink" title="ERNIE 2.0 Model"></a><a href="#ERNIE-2-0-Model" title="ERNIE 2.0 Model"></a>ERNIE 2.0 Model</h3><h4 id="Model-Structure"><a href="#Model-Structure" class="headerlink" title="Model Structure"></a><a href="#Model-Structure" title="Model Structure"></a>Model Structure</h4><ul>
<li>Transformer Encoder<ul>
<li>多层 Transformer</li>
<li>使用 self-attention 捕捉每个 token 的上下文信息</li>
<li>Sequence 开始位置添加 CLS（分类任务）标记，多任务之间用 SEP 标记分割</li>
</ul>
</li>
<li>Task Embedding</li>
</ul>
<p><img src="http://qnimg.lovevivian.cn/github-nlp-baidu-ernie-3.jpeg" alt></p>
<h4 id="Pre-training-Tasks"><a href="#Pre-training-Tasks" class="headerlink" title="Pre-training Tasks"></a><a href="#Pre-training-Tasks" title="Pre-training Tasks"></a>Pre-training Tasks</h4><ul>
<li>Word-aware Pre-training Tasks<ul>
<li>Knowledge Masking Task: 短语和命名实体 mask，初始化模型。</li>
<li>Capitalization Prediction Task: 判断一个英文单词是否大写。</li>
<li>Token-Document Relation Prediction Task: 预测某段中的 token 是否出现在原文的其他部分。根据经验，出现在文档许多部分中的单词通常是常用单词或与文档主题相关。因此通过识别段中的文档关键词可以在一定程度上捕获文档的关键词。</li>
</ul>
</li>
<li>Structure-aware Pre-training Tasks<ul>
<li>Sentence Reordering Task: 学习句子间关系。一个段落分成 m 小段后 random shuffle，然后让模型识别打乱的顺序（分类任务）。</li>
<li>Sentence Distance Task: 使用文档级别信息学习句子的距离。一个三分类任务：0 表示两个句子在一个文档中且相邻；1 表示在一个文档不相邻；2 表示不在一个文档。</li>
</ul>
</li>
<li>Semantic-aware Pre-training Tasks<ul>
<li>Discourse Relation Task: 预测两个句子的语义或修辞关系。</li>
<li>IR Relevance Task: 学习 IR 中短文本相关性，一个三分类任务预测 query 和 title 之间的关系：0 表示强相关，用户输入 query 后点击了 title；1 表示弱相关，用户输入 query，title 在搜索结果中但用户未点击；2 表示不相关。</li>
</ul>
</li>
</ul>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a><a href="#Experiments" title="Experiments"></a>Experiments</h3><p>主要与 Bert 和 XLNet 对比。</p>
<h4 id="Pre-training-and-Implementation"><a href="#Pre-training-and-Implementation" class="headerlink" title="Pre-training and Implementation"></a><a href="#Pre-training-and-Implementation" title="Pre-training and Implementation"></a>Pre-training and Implementation</h4><ul>
<li><p>Pre-training Data</p>
<p><img src="http://qnimg.lovevivian.cn/github-baidu-nlp-ernie-3.jpeg" alt></p>
</li>
<li><p>Pre-training Settings</p>
<ul>
<li>base model: 12 layers, 12 self-attention heads and 768-dimensional of hidden size</li>
<li>large model: 24 layers, 16 self-attention heads and 1024-dimensional of hidden size</li>
<li>Adam optimizer: β1=0.9, β2=0.98</li>
<li>Batch size: 393216 tokens</li>
<li>Learning rate: 5e-5 (English model), 1.28e-4 (Chinese model)</li>
<li>Scheduled by decay scheme noam with warmup over the first 4000 steps for every task</li>
</ul>
</li>
</ul>
<h4 id="Fine-tuning-Tasks"><a href="#Fine-tuning-Tasks" class="headerlink" title="Fine-tuning Tasks"></a><a href="#Fine-tuning-Tasks" title="Fine-tuning Tasks"></a>Fine-tuning Tasks</h4><ul>
<li><p>English</p>
<ul>
<li>CoLA: 用于判断句子是否符合语法规范</li>
<li>SST-2: 情感分析</li>
<li>MNLI: 文本推理</li>
<li>RTE: 自然语言推理</li>
<li>WNLI: 段落间相互信息</li>
<li>QQP: 判断问答对是否重复</li>
<li>MRPC: 句子对语义相关性</li>
<li>STS-B: 数据集，包括插图说明、新闻标题和论坛文字</li>
<li>QNLI: 判断给定的文本对是否是 问题 - 答案对</li>
<li><p>AX: 对模型进行语言分析</p>
<p><img src="http://qnimg.lovevivian.cn/github-baidu-nlp-ernie-4.jpeg" alt></p>
</li>
</ul>
</li>
<li><p>Chinese Tasks</p>
<ul>
<li>阅读理解: CMRC 2018, DRCD, DuReader</li>
<li>命名实体识别: MSRA-NER (SIGHAN 2006)</li>
<li>自然语言推理: XNLI</li>
<li>情感分析: ChnSentiCorp</li>
<li>语义相似度: LCQMC, BQ Corpus</li>
<li><p>问答: NLPCC-DBQA</p>
<p><img src="http://qnimg.lovevivian.cn/github-baidu-nlp-ernie-5.jpeg" alt></p>
</li>
</ul>
</li>
</ul>
<h4 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a><a href="#Implementation-Details" title="Implementation Details"></a>Implementation Details</h4><p><img src="http://qnimg.lovevivian.cn/github-baidu-nlp-ernie-6.jpeg" alt></p>
<p><img src="http://qnimg.lovevivian.cn/github-baidu-nlp-ernie-7.jpeg" alt></p>
<h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a><a href="#实践" title="实践"></a>实践</h2><p>官方<a href="https://github.com/PaddlePaddle/ERNIE/blob/develop/README.zh.md" target="_blank" rel="noopener">文档</a>还是比较清晰的，我觉得对于大多数公司和个人来说，还是不要自己去训练了，实在太费钱了。不过如果是领域性很强的任务，还是可以重新训练一下的，要不然模型太 general。</p>
<h3 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a><a href="#Fine-tuning" title="Fine-tuning"></a>Fine-tuning</h3><p>现在来到 “使用” 部分，这个依赖百度的 Paddle，可以通过官方文档先安装好。目前 2.0 的中文模型还没有发布，不过估计用不了多久就有了。其实我们需要做的就是 Fine-tuning，也就是利用官方的预训练模型 + 我们自己特定任务的数据得到 “符合” 我们自己特定任务的模型。不同类型的任务参数不一样，可以根据自己的任务类型选择特定的参数，参数在上面提到的官方文档中可以找到。</p>
<p>官方给了三个常用的例子：</p>
<ul>
<li>分类任务</li>
<li>序列标注任务</li>
<li>阅读理解任务</li>
</ul>
<p>我们只需设置好 data 和 model 路径（可以用 <a href="https://github.com/inishchith/autoenv" target="_blank" rel="noopener">autoenv</a>，比较方便），然后按文档说明运行 scripts 下的脚本即可。友好提醒下，如果你电脑没有 GPU 的话就不要运行了，几乎肯定会死机（我已经试过了，成功运行了，然后只能强制重启了……）。当然配置比较渣（Mac Pro 2015）：</p>
<ul>
<li>2.7 GHz Intel Core i5</li>
<li>8GB 1867 MHz DDR3</li>
<li>Intel Iris Graphics 6100 1536 MB</li>
</ul>
<p>Fine-tuning 方面没啥特别的，只要你的输入和给的数据集一样即可。另外，官方文档还给出了获取特定任务句子和词 Embedding 的代码，可参考 FAQ1。</p>
<p>模型训练好后需要进行预测，官方也给出了 <code>predict_classifier.py</code>，直接使用即可，使用方式可以参见文档的 FAQ2，感觉稍微弄弄就可以直接 deploy 的节奏。</p>
<h3 id="Pre-training"><a href="#Pre-training" class="headerlink" title="Pre-training"></a><a href="#Pre-training" title="Pre-training"></a>Pre-training</h3><p>刚刚说了，如果领域性比较强的话还是建议 pre-train 一下的，这个也很方便，我们只需要把数据处理成 ERNIE 要的格式即可（每行训练数据包括 5 个分号分割的字段）：</p>
<ul>
<li>token_ids: 输入句子对 token 的 id</li>
<li>sentence_type_ids: 输入句子对标签，0 和 1</li>
<li>position_ids: 输入句子对 token 的位置</li>
<li>seg_labels: 分词边界信息，0 表示词首、1 表示非词首、-1 为占位符</li>
<li>next_sentence_label: 输入的句子对是否存在上下句关系，0 表示无 1 表示有</li>
</ul>
<p>然后将 scripts 下脚本中的数据集换成自己的即可开始训练。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a><a href="#小结" title="小结"></a>小结</h2><p>目前官方给出的代码其实是 v1.0 的版本，所以 pre-train 的也是 1.0 版本，1.0 只有两个任务，对应：lm_loss 和 next_sent_loss，2.0 中文版会有更多的 loss。源代码也大概看了下，涉及很多不太熟悉的 Paddle，重点看了输入和 loss，感觉比较重要的就是 mask 和 negtive samples 的代码，分别在 <code>batching.py</code> 和 <code>pretraining.py</code> 中。当然，实际应用角度来看，其实我们只要搞定推理部分即可，即使要自己训练，也可以直接用官方给出的参数，调参还是交给有钱的大公司吧 :D</p>
<p>再次感慨下 Pre-training 在 NLP 领域的如火如荼，有钱的巨头们不断砸出更好的 model，我等吃瓜群众默默拿来使用，顺便给他们点个赞。最后说句题外话，看了下 ERNIE 的 Issue，我觉得官方应该给一个 <a href="https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/master/README-zh_CN.md" target="_blank" rel="noopener">How-To-Ask-Questions-The-Smart-Way</a> ：）</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a><a href="#参考" title="参考"></a>参考</h2><ul>
<li><a href="https://github.com/PaddlePaddle/ERNIE/blob/develop/README.zh.md" target="_blank" rel="noopener">ERNIE/README.zh.md at develop · PaddlePaddle/ERNIE</a></li>
<li><a href="https://arxiv.org/abs/1907.12412v1" target="_blank" rel="noopener">[1907.12412v1] ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</a></li>
<li><a href="https://yam.gift/2019/08/02/Paper/2019-08-02-Baidu-ERNIE-Tutorial/" target="_blank" rel="noopener">ERNIE 2.0 论文笔记+实践</a></li>
</ul>

    </div>

    
    
    
        
      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>AILab-aida</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://ailab-aida.github.io/2019/10/21/ERNIE 2.0/" title="ERNIE 2.0">https://ailab-aida.github.io/2019/10/21/ERNIE 2.0/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/算法/" rel="tag"># 算法</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/10/18/me/" rel="next" title="">
                  <i class="fa fa-chevron-left"></i> 
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/10/22/faiss 三种基础索引/" rel="prev" title="faiss 三种基础索引">
                  faiss 三种基础索引 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="gitalk-container"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#论文"><span class="nav-number">1.</span> <span class="nav-text">论文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract"><span class="nav-number">1.1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction"><span class="nav-number">1.2.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Unsupervised-Transfer-Learning-for-Language-Representation"><span class="nav-number">1.2.1.</span> <span class="nav-text">Unsupervised Transfer Learning for Language Representation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Continual-Learning"><span class="nav-number">1.2.2.</span> <span class="nav-text">Continual Learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-ERNIE-2-0-Framework"><span class="nav-number">1.3.</span> <span class="nav-text">The ERNIE 2.0 Framework</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Continual-Pre-training"><span class="nav-number">1.3.1.</span> <span class="nav-text">Continual Pre-training</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fine-tuning-for-Application-Tasks"><span class="nav-number">1.3.2.</span> <span class="nav-text">Fine-tuning for Application Tasks</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ERNIE-2-0-Model"><span class="nav-number">1.4.</span> <span class="nav-text">ERNIE 2.0 Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-Structure"><span class="nav-number">1.4.1.</span> <span class="nav-text">Model Structure</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pre-training-Tasks"><span class="nav-number">1.4.2.</span> <span class="nav-text">Pre-training Tasks</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Experiments"><span class="nav-number">1.5.</span> <span class="nav-text">Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Pre-training-and-Implementation"><span class="nav-number">1.5.1.</span> <span class="nav-text">Pre-training and Implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fine-tuning-Tasks"><span class="nav-number">1.5.2.</span> <span class="nav-text">Fine-tuning Tasks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Implementation-Details"><span class="nav-number">1.5.3.</span> <span class="nav-text">Implementation Details</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实践"><span class="nav-number">2.</span> <span class="nav-text">实践</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fine-tuning"><span class="nav-number">2.1.</span> <span class="nav-text">Fine-tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pre-training"><span class="nav-number">2.2.</span> <span class="nav-text">Pre-training</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">3.</span> <span class="nav-text">小结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">4.</span> <span class="nav-text">参考</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.png"
      alt="AILab-aida">
  <p class="site-author-name" itemprop="name">AILab-aida</p>
  <div class="site-description" itemprop="description">涉猎的主要编程语言为 深度学习、机器学习、大数据、服务端、移动端、前端、爬虫(go、scala、Java、flutter、Python、react、Vue)等。</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">56</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/qq1074123922" title="GitHub &rarr; https://github.com/qq1074123922" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="/1074123922@qq.com" title="E-Mail &rarr; 1074123922@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AILab-aida</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.0</div>

        












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  <script src="/lib/pjax/pjax.min.js?v=0.2.8"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[pjax], script#page-configurations, #pjax script').forEach(element => {
    var id = element.id || '';
    var src = element.src || '';
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (id !=='') {
      script.id = element.id;
    }
    if (src !== '') {
      script.src = src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  








  <script src="/js/local-search.js?v=7.4.0"></script>













    <div id="pjax">

  

  

  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: 'a6d340a24e0f5044ffc3',
      clientSecret: 'edff6432acd3e21caff2696cc123e15b3ca3461c',
      repo: 'ailab-aida.github.io',
      owner: 'AILab-aida',
      admin: ['ailab'],
      id: '2807c089a276276c5ae8e25209bc39ff',
        language: 'zh-CN',
      
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script>

    </div>
</body>
</html>
