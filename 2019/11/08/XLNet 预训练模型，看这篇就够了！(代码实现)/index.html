<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=7.4.0">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg?v=7.4.0" color="#222">
  <link rel="alternate" href="/atom.xml" title="AILab-aida" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: true,
    lazyload: false,
    pangu: true,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="什么是 XLNet XLNet 是一个类似 BERT 的模型，而不是完全不同的模型。总之，XLNet 是一种通用的自回归预训练方法。它是 CMU 和 Google Brain 团队在 2019 年 6 月份发布的模型，最终，XLNet 在 20 个任务上超过了 BERT 的表现，并在 18 个任务上取得了当前最佳效果（state-of-the-art），包括机器问答、自然语言推断、情感分析和文档">
<meta name="keywords" content="算法">
<meta property="og:type" content="article">
<meta property="og:title" content="XLNet 预训练模型，看这篇就够了！(代码实现)">
<meta property="og:url" content="https://ailab-aida.github.io/2019/11/08/XLNet 预训练模型，看这篇就够了！(代码实现)/index.html">
<meta property="og:site_name" content="AILab-aida">
<meta property="og:description" content="什么是 XLNet XLNet 是一个类似 BERT 的模型，而不是完全不同的模型。总之，XLNet 是一种通用的自回归预训练方法。它是 CMU 和 Google Brain 团队在 2019 年 6 月份发布的模型，最终，XLNet 在 20 个任务上超过了 BERT 的表现，并在 18 个任务上取得了当前最佳效果（state-of-the-art），包括机器问答、自然语言推断、情感分析和文档">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2019/9/30/16d80192bbc1bd2e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2019/9/30/16d80192bc8e0bd2?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:image" content="https://juejin.im/equation?tex=x_1%2Cx_2%2C...%2Cx_%7Bn-1%7D">
<meta property="og:image" content="https://juejin.im/equation?tex=x_n">
<meta property="og:image" content="https://juejin.im/equation?tex=x_n">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2019/9/30/16d80192bd0d5c7e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2019/9/30/16d80192bd281412?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2019/9/30/16d80192d0808bc0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:image" content="https://user-gold-cdn.xitu.io/2019/9/24/16d621b9608e9250?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
<meta property="og:updated_time" content="2019-11-19T03:31:45.705Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="XLNet 预训练模型，看这篇就够了！(代码实现)">
<meta name="twitter:description" content="什么是 XLNet XLNet 是一个类似 BERT 的模型，而不是完全不同的模型。总之，XLNet 是一种通用的自回归预训练方法。它是 CMU 和 Google Brain 团队在 2019 年 6 月份发布的模型，最终，XLNet 在 20 个任务上超过了 BERT 的表现，并在 18 个任务上取得了当前最佳效果（state-of-the-art），包括机器问答、自然语言推断、情感分析和文档">
<meta name="twitter:image" content="https://user-gold-cdn.xitu.io/2019/9/30/16d80192bbc1bd2e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1">
  <link rel="canonical" href="https://ailab-aida.github.io/2019/11/08/XLNet 预训练模型，看这篇就够了！(代码实现)/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>XLNet 预训练模型，看这篇就够了！(代码实现) | AILab-aida</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AILab-aida</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">一个专注技术的组织</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-sitemap">
      
    

    <a href="/atom.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/qq1074123922" class="github-corner" title="AILab-aida GitHub" aria-label="AILab-aida GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://ailab-aida.github.io/2019/11/08/XLNet 预训练模型，看这篇就够了！(代码实现)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AILab-aida">
      <meta itemprop="description" content="涉猎的主要编程语言为 深度学习、机器学习、大数据、服务端、移动端、前端、爬虫(go、scala、Java、flutter、Python、react、Vue)等。">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AILab-aida">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">XLNet 预训练模型，看这篇就够了！(代码实现)

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-11-08 11:50:14" itemprop="dateCreated datePublished" datetime="2019-11-08T11:50:14+08:00">2019-11-08</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-19 11:31:45" itemprop="dateModified" datetime="2019-11-19T11:31:45+08:00">2019-11-19</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <ol>
<li>什么是 XLNet</li>
</ol><hr><p>XLNet 是一个类似 BERT 的模型，而不是完全不同的模型。总之，<strong>XLNet 是一种通用的自回归预训练方法</strong>。它是 CMU 和 Google Brain 团队在 2019 年 6 月份发布的模型，最终，XLNet 在 20 个任务上超过了 BERT 的表现，并在 18 个任务上取得了当前最佳效果（state-of-the-art），包括机器问答、自然语言推断、情感分析和文档排序。</p><a id="more"></a>


<p>作者表示，BERT 这样基于去噪自编码器的预训练模型可以很好地建模双向语境信息，性能优于基于自回归语言模型的预训练方法。然而，由于需要 mask 一部分输入，BERT 忽略了被 mask 位置之间的依赖关系，因此出现预训练和微调效果的差异（pretrain-finetune discrepancy）。</p>
<p>基于这些优缺点，该研究提出了一种泛化的自回归预训练模型 XLNet。XLNet 可以：</p>
<ol>
<li>通过最大化所有可能的因式分解顺序的对数似然，学习双向语境信息；</li>
<li>用自回归本身的特点克服 BERT 的缺点；</li>
<li><p>此外，XLNet 还融合了当前最优自回归模型 Transformer-XL 的思路。</p>
</li>
<li><p>自回归语言模型（Autoregressive LM）</p>
</li>
</ol>
<hr>
<p>在 ELMO／BERT 出来之前，大家通常讲的语言模型其实是根据上文内容预测下一个可能跟随的单词，就是常说的自左向右的语言模型任务，或者反过来也行，就是根据下文预测前面的单词，这种类型的 LM 被称为自回归语言模型。GPT 就是典型的自回归语言模型。ELMO 尽管看上去利用了上文，也利用了下文，但是本质上仍然是自回归 LM，这个跟模型具体怎么实现有关系。ELMO 是做了两个方向（从左到右以及从右到左两个方向的语言模型），但是是分别有两个方向的自回归 LM，然后把 LSTM 的两个方向的隐节点状态拼接到一起，来体现双向语言模型这个事情的。所以其实是两个自回归语言模型的拼接，本质上仍然是自回归语言模型。</p>
<p>自回归语言模型有优点有缺点：</p>
<p><strong>缺点</strong>是只能利用上文或者下文的信息，不能同时利用上文和下文的信息，当然，貌似 ELMO 这种双向都做，然后拼接看上去能够解决这个问题，因为融合模式过于简单，所以效果其实并不是太好。</p>
<p><strong>优点</strong>其实跟下游 NLP 任务有关，比如生成类 NLP 任务，比如文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程。而 Bert 这种 DAE 模式，在生成类 NLP 任务中，就面临训练过程和应用过程不一致的问题，导致生成类的 NLP 任务到目前为止都做不太好。</p>
<ol>
<li>自编码语言模型（Autoencoder LM）</li>
</ol>
<hr>
<p>自回归语言模型只能根据上文预测下一个单词，或者反过来，只能根据下文预测前面一个单词。相比而言，Bert 通过在输入 X 中随机 Mask 掉一部分单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被 Mask 掉的单词，如果你对 Denoising Autoencoder 比较熟悉的话，会看出，这确实是典型的 DAE 的思路。那些被 Mask 掉的单词就是在输入侧加入的所谓噪音。类似 Bert 这种预训练模式，被称为 DAE LM。</p>
<p>这种 DAE LM 的优缺点正好和自回归 LM 反过来，它能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文，这是好处。缺点是啥呢？主要在输入侧引入 [Mask] 标记，导致预训练阶段和 Fine-tuning 阶段不一致的问题，因为 Fine-tuning 阶段是看不到 [Mask] 标记的。DAE 吗，就要引入噪音，[Mask] 标记就是引入噪音的手段，这个正常。</p>
<p>XLNet 的出发点就是：能否融合自回归 LM 和 DAE LM 两者的优点。就是说如果站在自回归 LM 的角度，如何引入和双向语言模型等价的效果；如果站在 DAE LM 的角度看，它本身是融入双向语言模型的，如何抛掉表面的那个 [Mask] 标记，让预训练和 Fine-tuning 保持一致。当然，XLNet 还讲到了一个 Bert 被 Mask 单词之间相互独立的问题。</p>
<ol>
<li>XLNet 模型</li>
</ol>
<hr>
<h3 id="4-1-排列语言建模（Permutation-Language-Modeling）"><a href="#4-1-排列语言建模（Permutation-Language-Modeling）" class="headerlink" title="4.1 排列语言建模（Permutation Language Modeling）"></a>4.1 排列语言建模（Permutation Language Modeling）</h3><p>Bert 的自编码语言模型也有对应的缺点，就是 XLNet 在文中指出的：</p>
<ol>
<li>第一个预训练阶段因为采取引入 [Mask] 标记来 Mask 掉部分单词的训练模式，而 Fine-tuning 阶段是看不到这种被强行加入的 Mask 标记的，所以两个阶段存在使用模式不一致的情形，这可能会带来一定的性能损失；</li>
<li>另外一个是，Bert 在第一个预训练阶段，假设句子中多个单词被 Mask 掉，这些被 Mask 掉的单词之间没有任何关系，是条件独立的，而有时候这些单词之间是有关系的。</li>
</ol>
<p>上面两点是 XLNet 在第一个预训练阶段，相对 Bert 来说要解决的两个问题。</p>
<p>其实思路也比较简洁，可以这么思考：XLNet 仍然遵循两阶段的过程，第一个阶段是语言模型预训练阶段；第二阶段是任务数据 Fine-tuning 阶段。它主要希望改动第一个阶段，就是说不像 Bert 那种带 Mask 符号的 Denoising-autoencoder 的模式，而是采用自回归 LM 的模式。就是说，看上去输入句子 X 仍然是自左向右的输入，看到 Ti 单词的上文 Context_before，来预测 Ti 这个单词。但是又希望在 Context_before 里，不仅仅看到上文单词，也能看到 Ti 单词后面的下文 Context_after 里的下文单词，这样的话，Bert 里面预训练阶段引入的 Mask 符号就不需要了，于是在预训练阶段，看上去是个标准的从左向右过程，Fine-tuning 当然也是这个过程，于是两个环节就统一起来。当然，这是目标。剩下是怎么做到这一点的问题。</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/9/30/16d80192bbc1bd2e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt></p>
<p>首先，需要强调一点，尽管上面讲的是把句子 X 的单词排列组合后，再随机抽取例子作为输入，但是，实际上你是不能这么做的，因为 Fine-tuning 阶段你不可能也去排列组合原始输入。所以，就必须让预训练阶段的输入部分，看上去仍然是 x1,x2,x3,x4 这个输入顺序，但是可以在 Transformer 部分做些工作，来达成我们希望的目标。</p>
<p>具体而言，XLNet 采取了 Attention 掩码的机制，你可以理解为，当前的输入句子是 X，要预测的单词 Ti 是第 i 个单词，前面 1 到 i-1 个单词，在输入部分观察，并没发生变化，该是谁还是谁。但是在 Transformer 内部，通过 Attention 掩码，从 X 的输入单词里面，也就是 Ti 的上文和下文单词中，随机选择 i-1 个，放到 Ti 的上文位置中，把其它单词的输入通过 Attention 掩码隐藏掉，于是就能够达成我们期望的目标（当然这个所谓放到 Ti 的上文位置，只是一种形象的说法，其实在内部，就是通过 Attention Mask，把其它没有被选到的单词 Mask 掉，不让它们在预测单词 Ti 的时候发生作用，如此而已。看着就类似于把这些被选中的单词放到了上文 Context_before 的位置了）。</p>
<p>具体实现的时候，XLNet 是用 “双流自注意力模型” 实现的，细节可以参考论文，但是基本思想就如上所述，双流自注意力机制只是实现这个思想的具体方式，理论上，你可以想出其它具体实现方式来实现这个基本思想，也能达成让 Ti 看到下文单词的目标。</p>
<p>这里简单说下 “<strong>双流自注意力机制</strong>”，一个是内容流自注意力，其实就是标准的 Transformer 的计算过程；主要是引入了 Query 流自注意力，这个是干嘛的呢？其实就是用来代替 Bert 的那个 [Mask] 标记的，因为 XLNet 希望抛掉 [Mask] 标记符号，但是比如知道上文单词 x1,x2，要预测单词 x3，此时在 x3 对应位置的 Transformer 最高层去预测这个单词，但是输入侧不能看到要预测的单词 x3，Bert 其实是直接引入 [Mask] 标记来覆盖掉单词 x3 的内容的，等于说 [Mask] 是个通用的占位符号。而 XLNet 因为要抛掉 [Mask] 标记，但是又不能看到 x3 的输入，于是 Query 流，就直接忽略掉 x3 输入了，只保留这个位置信息，用参数 w 来代表位置的 embedding 编码。其实 XLNet 只是扔了表面的 [Mask] 占位符号，内部还是引入 Query 流来忽略掉被 Mask 的这个单词。和 Bert 比，只是实现方式不同而已。</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/9/30/16d80192bc8e0bd2?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt></p>
<p>上面讲的 Permutation Language Model 是 XLNet 的主要理论创新，所以介绍的比较多，从模型角度讲，这个创新还是挺有意思的，因为它开启了自回归语言模型如何引入下文的一个思路，相信对于后续工作会有启发。当然，XLNet 不仅仅做了这些，它还引入了其它的因素，也算是一个当前有效技术的集成体。感觉 <strong>XLNet 就是 Bert、GPT 2.0 和 Transformer XL 的综合体变身</strong>：</p>
<ol>
<li>首先，它通过 PLM(Permutation Language Model) 预训练目标，吸收了 Bert 的双向语言模型；</li>
<li>然后，GPT2.0 的核心其实是更多更高质量的预训练数据，这个明显也被 XLNet 吸收进来了；</li>
<li>再然后，Transformer XL 的主要思想也被吸收进来，它的主要目标是解决 Transformer 对于长文档 NLP 应用不够友好的问题。</li>
</ol>
<h3 id="4-2-Transformer-XL"><a href="#4-2-Transformer-XL" class="headerlink" title="4.2 Transformer XL"></a>4.2 Transformer XL</h3><p>目前在 NLP 领域中，处理语言建模问题有两种最先进的架构：RNN 和 Transformer。RNN 按照序列顺序逐个学习输入的单词或字符之间的关系，而 Transformer 则接收一整段序列，然后使用 self-attention 机制来学习它们之间的依赖关系。这两种架构目前来看都取得了令人瞩目的成就，但它们都局限在捕捉长期依赖性上。</p>
<p>为了解决这一问题，CMU 联合 Google Brain 在 2019 年 1 月推出的一篇新论文《Transformer-XL：Attentive Language Models beyond a Fixed-Length Context》同时结合了 RNN 序列建模和 Transformer 自注意力机制的优点，在输入数据的每个段上使用 Transformer 的注意力模块，并使用循环机制来学习连续段之间的依赖关系。</p>
<h4 id="4-2-1-vanilla-Transformer"><a href="#4-2-1-vanilla-Transformer" class="headerlink" title="4.2.1 vanilla Transformer"></a>4.2.1 vanilla Transformer</h4><p>为何要提这个模型？因为 Transformer-XL 是基于这个模型进行的改进。</p>
<p>Al-Rfou 等人基于 Transformer 提出了一种训练语言模型的方法，来根据之前的字符预测片段中的下一个字符。例如，它使用 <img src="https://juejin.im/equation?tex=x_1%2Cx_2%2C...%2Cx_%7Bn-1%7D" alt> 预测字符 <img src="https://juejin.im/equation?tex=x_n" alt>，而在 <img src="https://juejin.im/equation?tex=x_n" alt> 之后的序列则被 mask 掉。论文中使用 64 层模型，并仅限于处理 512 个字符这种相对较短的输入，因此它将输入分成段，并分别从每个段中进行学习，如下图所示。 在测试阶段如需处理较长的输入，该模型会在每一步中将输入向右移动一个字符，以此实现对单个字符的预测。</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/9/30/16d80192bd0d5c7e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt></p>
<p>该模型在常用的数据集如 enwik8 和 text8 上的表现比 RNN 模型要好，但它仍有以下缺点：</p>
<ul>
<li><strong>上下文长度受限</strong>：字符之间的最大依赖距离受输入长度的限制，模型看不到出现在几个句子之前的单词。</li>
<li><strong>上下文碎片</strong>：对于长度超过 512 个字符的文本，都是从头开始单独训练的。段与段之间没有上下文依赖性，会让训练效率低下，也会影响模型的性能。</li>
<li><strong>推理速度慢</strong>：在测试阶段，每次预测下一个单词，都需要重新构建一遍上下文，并从头开始计算，这样的计算速度非常慢。</li>
</ul>
<h4 id="4-2-2-Transformer-XL"><a href="#4-2-2-Transformer-XL" class="headerlink" title="4.2.2 Transformer XL"></a>4.2.2 Transformer XL</h4><p>Transformer-XL 架构在 vanilla Transformer 的基础上引入了两点创新：循环机制（Recurrence Mechanism）和相对位置编码（Relative Positional Encoding），以克服 vanilla Transformer 的缺点。与 vanilla Transformer 相比，Transformer-XL 的另一个优势是它可以被用于单词级和字符级的语言建模。</p>
<ol>
<li><p><strong>引入循环机制</strong></p>
<p>与 vanilla Transformer 的基本思路一样，Transformer-XL 仍然是使用分段的方式进行建模，但其与 vanilla Transformer 的本质不同是在于引入了段与段之间的循环机制，使得当前段在建模的时候能够利用之前段的信息来实现长期依赖性。如下图所示：</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/9/30/16d80192bd281412?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt></p>
<p>在训练阶段，处理后面的段时，每个隐藏层都会接收两个输入：</p>
<ul>
<li>该段的前面隐藏层的输出，与 vanilla Transformer 相同（上图的灰色线）。</li>
<li>前面段的隐藏层的输出（上图的绿色线），可以使模型创建长期依赖关系。</li>
</ul>
<p>这两个输入会被拼接，然后用于计算当前段的 Key 和 Value 矩阵。</p>
<p>该方法可以利用前面更多段的信息，测试阶段也可以获得更长的依赖。在测试阶段，与 vanilla Transformer 相比，其速度也会更快。在 vanilla Transformer 中，一次只能前进一个 step，并且需要重新构建段，并全部从头开始计算；而在 Transformer-XL 中，每次可以前进一整个段，并利用之前段的数据来预测当前段的输出。</p>
</li>
<li><p><strong>相对位置编码</strong></p>
<p>在 Transformer 中，一个重要的地方在于其考虑了序列的位置信息。在分段的情况下，如果仅仅对于每个段仍直接使用 Transformer 中的位置编码，即每个不同段在同一个位置上的表示使用相同的位置编码，就会出现问题。比如，第 i−2i-2i−2 段和第 i−1i-1i−1 段的第一个位置将具有相同的位置编码，但它们对于第 iii 段的建模重要性显然并不相同（例如第 i−2i-2i−2 段中的第一个位置重要性可能要低一些）。因此，需要对这种位置进行区分。</p>
<p>论文对于这个问题，提出了一种新的位置编码的方式，即会根据词之间的相对距离而非像 Transformer 中的绝对位置进行编码。从另一个角度来解读公式的话，可以将 attention 的计算分为如下四个部分：</p>
<ul>
<li>基于内容的 “寻址”，即没有添加原始位置编码的原始分数。</li>
<li>基于内容的位置偏置，即相对于当前内容的位置偏差。</li>
<li>全局的内容偏置，用于衡量 key 的重要性。</li>
<li>全局的位置偏置，根据 query 和 key 之间的距离调整重要性。</li>
</ul>
<p>详细公式见：<a href="https://blog.csdn.net/magical_bubble/article/details/89060213" target="_blank" rel="noopener">Transformer-XL 解读（论文 + PyTorch 源码）</a></p>
</li>
</ol>
<p>5) XLNet 与 BERT 比较</p>
<hr>
<p>尽管看上去，XLNet 在预训练机制引入的 Permutation Language Model 这种新的预训练目标，和 Bert 采用 Mask 标记这种方式，有很大不同。其实你深入思考一下，会发现，两者本质是类似的。</p>
<p><strong>区别主要在于</strong>：</p>
<ul>
<li>Bert 是直接在输入端显示地通过引入 Mask 标记，在输入侧隐藏掉一部分单词，让这些单词在预测的时候不发挥作用，要求利用上下文中其它单词去预测某个被 Mask 掉的单词；</li>
<li>而 XLNet 则抛弃掉输入侧的 Mask 标记，通过 Attention Mask 机制，在 Transformer 内部随机 Mask 掉一部分单词（这个被 Mask 掉的单词比例跟当前单词在句子中的位置有关系，位置越靠前，被 Mask 掉的比例越高，位置越靠后，被 Mask 掉的比例越低），让这些被 Mask 掉的单词在预测某个单词的时候不发生作用。</li>
</ul>
<p>所以，本质上两者并没什么太大的不同，只是 Mask 的位置，Bert 更表面化一些，XLNet 则把这个过程隐藏在了 Transformer 内部而已。这样，就可以抛掉表面的 [Mask] 标记，解决它所说的预训练里带有 [Mask] 标记导致的和 Fine-tuning 过程不一致的问题。至于说 XLNet 说的，Bert 里面被 Mask 掉单词的相互独立问题，也就是说，在预测某个被 Mask 单词的时候，其它被 Mask 单词不起作用，这个问题，你深入思考一下，其实是不重要的，因为 XLNet 在内部 Attention Mask 的时候，也会 Mask 掉一定比例的上下文单词，只要有一部分被 Mask 掉的单词，其实就面临这个问题。而如果训练数据足够大，其实不靠当前这个例子，靠其它例子，也能弥补被 Mask 单词直接的相互关系问题，因为总有其它例子能够学会这些单词的相互依赖关系。</p>
<p>当然，XLNet 这种改造，维持了表面看上去的自回归语言模型的从左向右的模式，这个 Bert 做不到，这个有明显的好处，就是对于生成类的任务，能够在维持表面从左向右的生成过程前提下，模型里隐含了上下文的信息。所以看上去，XLNet 貌似应该对于生成类型的 NLP 任务，会比 Bert 有明显优势。另外，因为 XLNet 还引入了 Transformer XL 的机制，所以对于长文档输入类型的 NLP 任务，也会比 Bert 有明显优势。</p>
<ol>
<li>代码实现</li>
</ol>
<hr>
<p><a href="https://github.com/ymcui/Chinese-PreTrained-XLNet" target="_blank" rel="noopener">中文 XLNet 预训练模型</a></p>
<p>【<a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">机器学习通俗易懂系列文章</a>】</p>
<p><img src="https://user-gold-cdn.xitu.io/2019/9/30/16d80192d0808bc0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt></p>
<ol>
<li>参考文献</li>
</ol>
<hr>
<ul>
<li><a href="https://blog.csdn.net/weixin_37947156/article/details/93035607" target="_blank" rel="noopener">XLNet 原理解读</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/70257427" target="_blank" rel="noopener">XLNet: 运行机制及和 Bert 的异同比较</a></li>
<li><a href="https://blog.csdn.net/magical_bubble/article/details/89060213" target="_blank" rel="noopener">Transformer-XL 解读（论文 + PyTorch 源码）</a></li>
</ul>
<blockquote>
<p>作者:<a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">@mantchs</a></p>
<p>GitHub:<a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">github.com/NLP-LOVE/ML…</a></p>
<p>欢迎大家加入讨论！共同完善此项目！群号:【541954936】<a href="//shang.qq.com/wpa/qunwpa?idkey=863f915b9178560bd32ca07cd090a7d9e6f5f90fcff5667489697b1621cecdb3"><img src="https://user-gold-cdn.xitu.io/2019/9/24/16d621b9608e9250?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt></a></p>
</blockquote>

    </div>

    
    
    
        
      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>AILab-aida</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://ailab-aida.github.io/2019/11/08/XLNet 预训练模型，看这篇就够了！(代码实现)/" title="XLNet 预训练模型，看这篇就够了！(代码实现)">https://ailab-aida.github.io/2019/11/08/XLNet 预训练模型，看这篇就够了！(代码实现)/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/算法/" rel="tag"># 算法</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/11/08/BERT 预训练模型的演进过程！(附代码)/" rel="next" title="BERT 预训练模型的演进过程！(附代码)">
                  <i class="fa fa-chevron-left"></i> BERT 预训练模型的演进过程！(附代码)
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/11/08/textRNN & textCNN 的网络结构与代码实现！/" rel="prev" title="textRNN & textCNN 的网络结构与代码实现！">
                  textRNN & textCNN 的网络结构与代码实现！ <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="gitalk-container"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-排列语言建模（Permutation-Language-Modeling）"><span class="nav-number">1.</span> <span class="nav-text">4.1 排列语言建模（Permutation Language Modeling）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Transformer-XL"><span class="nav-number">2.</span> <span class="nav-text">4.2 Transformer XL</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-1-vanilla-Transformer"><span class="nav-number">2.1.</span> <span class="nav-text">4.2.1 vanilla Transformer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-2-Transformer-XL"><span class="nav-number">2.2.</span> <span class="nav-text">4.2.2 Transformer XL</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.png"
      alt="AILab-aida">
  <p class="site-author-name" itemprop="name">AILab-aida</p>
  <div class="site-description" itemprop="description">涉猎的主要编程语言为 深度学习、机器学习、大数据、服务端、移动端、前端、爬虫(go、scala、Java、flutter、Python、react、Vue)等。</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">109</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/qq1074123922" title="GitHub &rarr; https://github.com/qq1074123922" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="/1074123922@qq.com" title="E-Mail &rarr; 1074123922@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AILab-aida</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.0</div>

        












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  <script src="/lib/pjax/pjax.min.js?v=0.2.8"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[pjax], script#page-configurations, #pjax script').forEach(element => {
    var id = element.id || '';
    var src = element.src || '';
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (id !=='') {
      script.id = element.id;
    }
    if (src !== '') {
      script.src = src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  








  <script src="/js/local-search.js?v=7.4.0"></script>













    <div id="pjax">

  

  

  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: 'a6d340a24e0f5044ffc3',
      clientSecret: 'edff6432acd3e21caff2696cc123e15b3ca3461c',
      repo: 'ailab-aida.github.io',
      owner: 'AILab-aida',
      admin: ['ailab'],
      id: 'f0e08f33a2bcad7f179c515ef316ffd3',
        language: 'zh-CN',
      
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script>

    </div>
</body>
</html>
