<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=7.4.0">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg?v=7.4.0" color="#222">
  <link rel="alternate" href="/atom.xml" title="AILab-aida" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: true,
    lazyload: false,
    pangu: true,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="深度学习训练营课程笔记。References机器学习初等指南 (1)机器学习初等指南 (2)机器学习初等指南 (3)参考课件. zipAI 研习社深度学习 Tensorflow 相关书籍推荐和 PDF 下载深度学习入门：基于 Python 的理论与实现逻辑回归逻辑回归的内容可以回顾这里：机器学习初等指南 (1)- 逻辑回归。">
<meta name="keywords" content="算法">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习训练营 _ 思维之海">
<meta property="og:url" content="https://ailab-aida.github.io/2019/11/08/深度学习训练营 _ 思维之海/index.html">
<meta property="og:site_name" content="AILab-aida">
<meta property="og:description" content="深度学习训练营课程笔记。References机器学习初等指南 (1)机器学习初等指南 (2)机器学习初等指南 (3)参考课件. zipAI 研习社深度学习 Tensorflow 相关书籍推荐和 PDF 下载深度学习入门：基于 Python 的理论与实现逻辑回归逻辑回归的内容可以回顾这里：机器学习初等指南 (1)- 逻辑回归。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://vel.life/深度学习训练营/1549954237865.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1549956520039.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1550014524166.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1550015036266.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1550015239822.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1550013924570.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1550024164080.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1550045365465.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1550046740424.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1550046862286.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1550051038255.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1550053048679.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/v2-a5b5870ab0a4030684c7f5796e3b83a6_b.gif">
<meta property="og:image" content="https://vel.life/深度学习训练营/5c6bb4c6ce26f.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/5c6bb64e34dd7.gif">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bc30241434.gif?imageView2/2/w/740">
<meta property="og:image" content="https://vel.life/深度学习训练营/5c6bba459d2e1.gif">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bba4ac0bfb.gif?imageView2/2/w/740">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bba50054a1.png?imageView2/2/w/740">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bbc17a3cb9.png?imageView2/2/w/740">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bbc1e7476d.png?imageView2/2/w/740">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bbc23df89f.png?imageView2/2/w/740">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bc28a260ef.gif?imageView2/2/w/740">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bc32c58088.gif?imageView2/2/w/740">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bc3949d069.jpeg?imageView2/2/w/740">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bc394991e7.png?imageView2/2/w/740">
<meta property="og:image" content="https://static.leiphone.com/uploads/new/images/20190219/5c6bc939c515b.gif?imageView2/2/w/740">
<meta property="og:image" content="https://vel.life/深度学习训练营/block.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/table.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/VGG16.png">
<meta property="og:image" content="http://deanhan.com/2018/07/26/vgg16/ppt.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1553319814542.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1553320192995.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1553320223350.png">
<meta property="og:image" content="https://vel.life/深度学习训练营/1553327994360.png">
<meta property="og:updated_time" content="2019-11-19T03:41:01.388Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习训练营 _ 思维之海">
<meta name="twitter:description" content="深度学习训练营课程笔记。References机器学习初等指南 (1)机器学习初等指南 (2)机器学习初等指南 (3)参考课件. zipAI 研习社深度学习 Tensorflow 相关书籍推荐和 PDF 下载深度学习入门：基于 Python 的理论与实现逻辑回归逻辑回归的内容可以回顾这里：机器学习初等指南 (1)- 逻辑回归。">
<meta name="twitter:image" content="https://vel.life/深度学习训练营/1549954237865.png">
  <link rel="canonical" href="https://ailab-aida.github.io/2019/11/08/深度学习训练营 _ 思维之海/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>深度学习训练营 _ 思维之海 | AILab-aida</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AILab-aida</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">一个专注技术的组织</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-sitemap">
      
    

    <a href="/atom.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/qq1074123922" class="github-corner" title="AILab-aida GitHub" aria-label="AILab-aida GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://ailab-aida.github.io/2019/11/08/深度学习训练营 _ 思维之海/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="AILab-aida">
      <meta itemprop="description" content="涉猎的主要编程语言为 深度学习、机器学习、大数据、服务端、移动端、前端、爬虫(go、scala、Java、flutter、Python、react、Vue)等。">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AILab-aida">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">深度学习训练营 _ 思维之海

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-11-08 11:52:25" itemprop="dateCreated datePublished" datetime="2019-11-08T11:52:25+08:00">2019-11-08</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-19 11:41:01" itemprop="dateModified" datetime="2019-11-19T11:41:01+08:00">2019-11-19</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a href="http://www.xuetangx.com/courses/course-v1:xuetangx+deeplearningxly+2018_T1/about" target="_blank" rel="noopener">深度学习训练营</a>课程笔记。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a><a href="#References" title="References"></a>References</h1><p><a href="https://vel.life/Notes-of-Machine-Learning-Part1/" target="_blank" rel="noopener">机器学习初等指南 (1)</a></p><p><a href="https://vel.life/Notes-of-Machine-Learning-Part2/" target="_blank" rel="noopener">机器学习初等指南 (2)</a></p><p><a href="https://vel.life/Notes-of-Machine-Learning-Part3/" target="_blank" rel="noopener">机器学习初等指南 (3)</a></p><p><a href="./参考课件.zip">参考课件. zip</a></p><p><a href="https://ai.yanxishe.com/" target="_blank" rel="noopener">AI 研习社</a></p><p><a href="https://www.jianshu.com/p/a8c0266a7ba4" target="_blank" rel="noopener">深度学习 Tensorflow 相关书籍推荐和 PDF 下载</a></p><p><a href="./《深度学习入门：基于Python的理论与实现》高清中文版.pdf">深度学习入门：基于 Python 的理论与实现</a></p><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a><a href="#逻辑回归" title="逻辑回归"></a>逻辑回归</h1><p>逻辑回归的内容可以回顾这里：<a href="https://vel.life/Notes-of-Machine-Learning-Part1/#Logistic回归" target="_blank" rel="noopener">机器学习初等指南 (1)- 逻辑回归</a>。</p><a id="more"></a>








<p><strong>Ex</strong>：利用年龄和收入预测是否买车。</p>
<p><a href="/深度学习训练营/1549954237865.png"><img src="https://vel.life/深度学习训练营/1549954237865.png" alt></a></p>
<table><tbody><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

</pre></td><td class="code"><pre>from sklearn import linear_model #引入sklearn线性模型

X = [[20, 3],
[23, 7],
[31, 10],
[42, 13],
[50, 7],
[60, 5]] #年龄-收入矩阵
y = [0, 1, 1, 1 ,0 , 0] #观测向量

lr = linear_model.LogisticRegression() #创建对象
lr.fit(X,y) #获得拟合参数

testX = [[28, 8]] #测试集（矩阵）

label = lr.predict(testX) #预测
print("predicted Label = ", label)
prob = lr.predict_proba(testX) #预测概率
print("probability = ", prob)

</pre></td></tr></tbody></table>

<h2 id="拟合系数的含义"><a href="#拟合系数的含义" class="headerlink" title="拟合系数的含义"></a><a href="#拟合系数的含义" title="拟合系数的含义"></a>拟合系数的含义</h2><p>𝑃(𝑌=1|𝑥;𝜃)=𝑓(𝑥;𝜃)=11+𝑒−𝜃𝑇𝑥P(Y=1|x;θ)=f(x;θ)=11+e−θTx P(Y=1|x;\theta)=f(x;\theta)=\cfrac{1}{1+e^{-\theta^Tx}}</p>
<blockquote>
<p>则<strong>概率比值</strong>𝑜𝑑𝑑𝑠=𝑝1−𝑝=𝑒𝜃𝑇𝑥odds=p1−p=eθTxodds=\frac{p}{1-p}=e^{\theta^Tx}。</p>
<p>系数 𝜃𝑗 意味着，假设 𝑜𝑑𝑑𝑠 为 𝜆1(原),𝜆2(新)，若 𝑥𝑗 增加 1，有 𝜆2𝜆1=𝑒𝜃𝑗 系数 θj 意味着，假设 odds 为 λ1(原),λ2(新)，若 xj 增加 1，有 λ2λ1=eθj 系数\theta_j 意味着，假设 odds 为\lambda_1(原),\lambda_2(新)，若 x_j 增加 1，有\cfrac{\lambda_2}{\lambda_1}=e^{\theta_j}。</p>
<p>以下是验证程序：</p>
</blockquote>
<table><tbody><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39

</pre></td><td class="code"><pre>from sklearn import linear_model #引入sklearn线性模型

X = [[20, 3],
[23, 7],
[31, 10],
[42, 13],
[50, 7],
[60, 5]] #年龄-收入矩阵
y = [0, 1, 1, 1 ,0 , 0] #观测向量
lr = linear_model.LogisticRegression()
lr.fit(X,y) #获得拟合参数

testX = [[28, 8]] #测试集（矩阵）

label = lr.predict(testX) #预测
print("predicted Label = ", label)

prob = lr.predict*proba(testX) #预测概率
print("probability = ", prob)
#-----------------------------------------以下是新增部分
theta_0 = lr.intercept*
theta*1 = lr.coef*[0][0]
theta*2 = lr.coef*[0][1]
print("theta_0 = ", theta_0)
print("theta_1 = ", theta_1)
print("theta_2 = ", theta_2)

ratio = prob[0][1]/prob[0][0]

testX = [[28, 9]]
prob_new = lr.predict_proba(testX)
ratio_new = prob_new[0][1]/prob_new[0][0]

ratio_of_ratio = ratio_new / ratio
print("ratio_of_ratio = ", ratio_of_ratio)

import math
theta_2_exp = math.exp(theta_2)
print("theta_2_exp = ", theta_2_exp)

</pre></td></tr></tbody></table>

<p><a href="/深度学习训练营/1549956520039.png"><img src="https://vel.life/深度学习训练营/1549956520039.png" alt></a></p>
<h2 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a><a href="#应用案例" title="应用案例"></a>应用案例</h2><p>参考程序：<a href="./spam_detection.ipynb">spam_detection.ipynb</a>， <a href="./垃圾短信检测数据集.zip">垃圾短信检测数据集. zip</a></p>
<h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a><a href="#示例代码" title="示例代码"></a>示例代码</h3><p><a href="http://huanyouchen.github.io/2018/05/30/hexo-support-jupyter-notebook-in-blog/" target="_blank" rel="noopener">在 hexo 中写的文章支持 jupyter-notebook 显示 | 幻悠尘的小窝</a></p>
<blockquote>
<p>注意需要<code>npm install co</code>。</p>
<p>太偏右修正：<a href="https://github.com/qiliux/hexo-jupyter-notebook/issues/3。" target="_blank" rel="noopener">https://github.com/qiliux/hexo-jupyter-notebook/issues/3。</a></p>
<blockquote>
<p>核心代码：<code>document.getElementById(&#39;ipynb&#39;).style[&#39;margin-left&#39;] = &#39;-60px&#39;;</code></p>
</blockquote>
</blockquote>
<h3 id="简单代码"><a href="#简单代码" class="headerlink" title="简单代码"></a><a href="#简单代码" title="简单代码"></a>简单代码</h3><table><tbody><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

</pre></td><td class="code"><pre>import pandas as pd
from sklearn import linear_model
from sklearn.feature_extraction.text import TfidfVectorizer

df = pd.read_csv('spam.csv', delimiter=',', header=None, encoding='latin-1')
print(df.head(5))
y, X_train = df[0], df[1]

# df = pd.read_csv('spam.csv', delimiter=',', encoding='latin-1')

# y, X_train = df.iloc[:,0], df.iloc[:,1]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(X_train)

lr = linear_model.LogisticRegression()
lr.fit(X, y)

testX = vectorizer.transform(['URGENT! Your mobile No. 1234 was awarded a prize.',
'Hey honey, whats up?'])
predictions = lr.predict(testX)
print(predictions)

</pre></td></tr></tbody></table>

<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a><a href="#神经网络" title="神经网络"></a>神经网络</h1><p>神经网络的内容可以回顾这里：<a href="https://vel.life/Notes-of-Machine-Learning-Part2/#神经网络" target="_blank" rel="noopener">机器学习初等指南 (2)</a>。</p>
<h2 id="鸢尾花分类"><a href="#鸢尾花分类" class="headerlink" title="鸢尾花分类"></a><a href="#鸢尾花分类" title="鸢尾花分类"></a>鸢尾花分类</h2><blockquote>
<p><a href="https://www.jianshu.com/p/6ada344f91ce" target="_blank" rel="noopener">鸢尾花数据集</a>， <a href="https://www.wikiwand.com/en/Iris_flower_data_set" target="_blank" rel="noopener">Iris flower data set</a></p>
<blockquote>
<p><a href="https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv" target="_blank" rel="noopener">https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv</a></p>
</blockquote>
<p><a href="https://www.cnblogs.com/meelo/p/4272036.html" target="_blank" rel="noopener"><strong>探索 sklearn | 鸢尾花数据集</strong></a></p>
<p><a href="https://blog.csdn.net/caojianhua2018/article/details/78658180" target="_blank" rel="noopener">机器学习 iris 数据集导入</a></p>
<p><a href="https://blog.csdn.net/Eastmount/article/details/78692227" target="_blank" rel="noopener">【python 数据挖掘课程】十九. 鸢尾花数据集可视化、线性回归、决策树花样分析</a></p>
<p><a href="https://www.jianshu.com/p/eff2df3984e1" target="_blank" rel="noopener">Iris 鸢尾花数据集可视化、线性回归、决策树分析、KMeans 聚类分析</a></p>
<blockquote>
<p>Iris plants 数据集可以从<code>KEEL dataset</code>数据集网站获取，也可以直接从<code>Sklearn.datasets</code>机器学习包得到。数据集共包含 4 个特征变量、1 个类别变量，共有 150 个样本。类别变量分别对应鸢尾花的三个亚属，分别是<code>山鸢尾 (Iris-setosa)</code>、<code>变色鸢尾(Iris-versicolor)</code>和<code>维吉尼亚鸢尾(Iris-virginica)</code> 分别用<code>[0,1,2]</code>来做映射。</p>
</blockquote>
<p><a href="https://www.jianshu.com/p/735e5de6bb6d" target="_blank" rel="noopener">小蛇学 python（14）K-means 预测花朵种类</a></p>
</blockquote>
<table><tbody><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67

</pre></td><td class="code"><pre># -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
from keras.models import Sequential #序列串行类
from keras.layers import Dense #密集层（全连接神经网络）
from keras.wrappers.scikit_learn import KerasClassifier #与sklearn的包装接口
from keras.utils import np_utils #
from sklearn.model_selection import cross_val_score # 交叉验证准确度得分
from sklearn.model_selection import KFold # k折交叉验证
from sklearn.preprocessing import LabelEncoder # 标签编码（转化为数值，类似向量化）
from keras.models import model_from_json # 训练模型的储存

# reproducibility

seed = 13 # 伪随机数种子
np.random.seed(seed)

# load data

df = pd.read_csv('iris.csv')

# from sklearn.datasets import load_iris

# df = load_iris()

X = df.values[:,0:4].astype(float) #设定为单精度浮点类型
Y = df.values[:,4]

encoder = LabelEncoder() #新建编码器
Y_encoded = encoder.fit_transform(Y) #进行数值转换
Y_onehot = np_utils.to_categorical(Y_encoded) #转化为向量

#define a network
def baseline_model():
model = Sequential()
model.add(Dense(7, input_dim=4, activation='tanh')) #新建(隐藏)层：Dense 全连接，7 个神经元，输入维度为 4，激活函数为 tanh 双曲正切
model.add(Dense(3, activation='softmax')) #新建输出层，3 个输出，激活函数为 softmax
model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy']) #损失函数：最小均方。优化方法：随机梯度下降。评估指标：准确度
return model

estimator = KerasClassifier(build_fn=baseline_model, epochs=20, batch_size=1, verbose=1)

# 交叉验证。 build_fn：模型生成方法。epochs：训练次数。batchsize：批处理容量。verbose：输出信息详细度。

# epchs 如果增加到 200，准确率可达 97%。

#evalute
kfold = KFold(n_splits=10, shuffle=True, random_state=seed)

# n_split：将数据集分成 10 份。shuffle：乱序预处理。

result = cross_val_score(estimator, X, Y_onehot, cv=kfold)
print("Accuray of cross validation, mean %.2f,std %.2f" % (result.mean(), result.std())) #均值、方差

#save model
estimator.fit(X, Y_onehot)
model_json = estimator.model.to_json()
with open("model.json", "w") as json_file: # 保存结构
json_file.write(model_json)

estimator.model.save_weights("model.h5") # 保存数值
print("save model to disk")

# load model and use it for prediction

json_file = open("model.json", "r")
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
loaded_model.load_weights("model.h5")
print("loaded model from dish")

predicted = loaded_model.predict(X)
print("predicted probability:" + str(predicted))

predicted_label = loaded_model.predict_classes(X)
print("predicted label:" + str(predicted_label))

</pre></td></tr></tbody></table>

<h3 id="一些知识"><a href="#一些知识" class="headerlink" title="一些知识"></a><a href="#一些知识" title="一些知识"></a>一些知识</h3><p><a href="https://www.wikiwand.com/zh-hans/Softmax%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">softmax 函数</a>：</p>
<p><a href="/深度学习训练营/1550014524166.png"><img src="https://vel.life/深度学习训练营/1550014524166.png" alt></a></p>
<blockquote>
<p><code>softmax</code>存在数值问题。（中间有<strong>指数膨胀</strong>）</p>
</blockquote>
<p><a href="/深度学习训练营/1550015036266.png"><img src="https://vel.life/深度学习训练营/1550015036266.png" alt></a></p>
<p>可以利用最大值偏置进行修正。避免溢出。</p>
<p><a href="/深度学习训练营/1550015239822.png"><img src="https://vel.life/深度学习训练营/1550015239822.png" alt></a></p>
<p><code>tanh</code>函数：</p>
<p><a href="/深度学习训练营/1550013924570.png"><img src="https://vel.life/深度学习训练营/1550013924570.png" alt></a></p>
<h2 id="深度神经网络"><a href="#深度神经网络" class="headerlink" title="深度神经网络"></a><a href="#深度神经网络" title="深度神经网络"></a>深度神经网络</h2><p>深度神经网络的两大挑战：</p>
<ul>
<li><p><strong>梯度消亡</strong>（Gradient Vanishing）：<strong>训练过程非常慢</strong>。</p>
</li>
<li><p><strong>过拟合</strong>（Overfitting）：在训练数据上效果好，在测试数据集上效果差。</p>
</li>
</ul>
<h3 id="梯度消亡"><a href="#梯度消亡" class="headerlink" title="梯度消亡"></a><a href="#梯度消亡" title="梯度消亡"></a>梯度消亡</h3><p>神经网络<strong>靠近输入端</strong>的网络层<strong>系数变化不敏感</strong>。当网络层数增加时，现象更明显。</p>
<p>梯度消亡的前提：</p>
<ul>
<li>使用<strong>基于梯度</strong>的训练方法</li>
<li>激活函数的输出值范围<strong>远小于</strong>输入值范围（<code>sigmoid</code>、<code>tanh</code>、<code>softmax</code>）</li>
</ul>
<blockquote>
<p>实际上，对于这样的激活函数，100、10000 和 10005 三者的激活值几乎一致。</p>
</blockquote>
<p>如果一个（大）系数的微小变化对网络的影响很小，那么就很难进行优化（优化特别慢）。训练起来就很困难了。</p>
<p>可以想象的是，损失函数是一个非常平坦的凹面。</p>
<h4 id="ReLU-激活"><a href="#ReLU-激活" class="headerlink" title="ReLU 激活"></a><a href="#ReLU激活" title="ReLU激活"></a>ReLU 激活</h4><ul>
<li><p><code>ReLU</code>:𝑓(𝑥)=𝑚𝑎𝑥(0,𝑥)f(x)=max(0,x)f(x)=max(0,x)。</p>
<ul>
<li>正值梯度。</li>
</ul>
</li>
<li><p><code>LeakyReLU</code>:𝑓(𝑥)=𝑚𝑎𝑥(𝑎𝑥,𝑥)f(x)=max(ax,x)f(x)=max(ax,x)。</p>
<ul>
<li>优化了负值梯度。</li>
</ul>
</li>
</ul>
<blockquote>
<p>为什么不直接选择激活函数 𝑓(𝑥)=𝑥f(x)=xf(x)=x（恒同映射）？</p>
<p>激活函数一定要是非线性的。如果激活函数是线性函数，那么最后得到的就是一个线性分类器。<br>激活函数的非线性越强，那么分类能力也就越强。</p>
</blockquote>
<h4 id="非梯度训练方法"><a href="#非梯度训练方法" class="headerlink" title="非梯度训练方法"></a><a href="#非梯度训练方法" title="非梯度训练方法"></a>非梯度训练方法</h4><p><a href="/深度学习训练营/1550024164080.png"><img src="https://vel.life/深度学习训练营/1550024164080.png" alt></a></p>
<h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a><a href="#过拟合" title="过拟合"></a>过拟合</h3><p>解决方案：</p>
<ul>
<li><code>Dropout</code></li>
<li><code>L2</code>正则化</li>
<li><code>L1</code>正则化</li>
<li><code>MaxNorm</code></li>
</ul>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a><a href="#Dropout" title="Dropout"></a><code>Dropout</code></h4><blockquote>
<p><a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></p>
<p><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=6&amp;ved=2ahUKEwjP1J7cobjgAhUEPXAKHRUMAzIQFjAFegQICRAB&amp;url=https%3A%2F%2Fblog.csdn.net%2Fqq_25011449%2Farticle%2Fdetails%2F81168369&amp;usg=AOvVaw3UXvWzAcK7FJEFgAuU-k9j" target="_blank" rel="noopener">【论文精读】Dropout: A Simple Way to Prevent Neural Networks from …</a>，<a href="https://me.csdn.net/qq_25011449" target="_blank" rel="noopener">Atlas8346</a></p>
</blockquote>
<p><a href="/深度学习训练营/1550045365465.png"><img src="https://vel.life/深度学习训练营/1550045365465.png" alt></a></p>
<p>Dropout 可以被解释为一种通过在隐藏的单元中添加噪声来调节神经网络的方法。</p>
<p>大概意思就是引入神经元的<strong>休息状态</strong>。神经元有一定概率被<code>Dropout</code>，也就完全不能被激活，从而输出全为<code>0</code>。</p>
<h4 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a><a href="#L2正则化" title="L2正则化"></a><code>L2</code>正则化</h4><p><a href="/深度学习训练营/1550046740424.png"><img src="https://vel.life/深度学习训练营/1550046740424.png" alt></a></p>
<h4 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a><a href="#L1正则化" title="L1正则化"></a><code>L1</code>正则化</h4><p><a href="/深度学习训练营/1550046862286.png"><img src="https://vel.life/深度学习训练营/1550046862286.png" alt></a></p>
<h4 id="MaxNorm"><a href="#MaxNorm" class="headerlink" title="MaxNorm"></a><a href="#MaxNorm" title="MaxNorm"></a><code>MaxNorm</code></h4><p><a href="/深度学习训练营/1550051038255.png"><img src="https://vel.life/深度学习训练营/1550051038255.png" alt></a></p>
<h3 id="神经网络系数的初始化"><a href="#神经网络系数的初始化" class="headerlink" title="神经网络系数的初始化"></a><a href="#神经网络系数的初始化" title="神经网络系数的初始化"></a>神经网络系数的初始化</h3><p><a href="/深度学习训练营/1550053048679.png"><img src="https://vel.life/深度学习训练营/1550053048679.png" alt></a></p>
<h1 id="实战项目-1-自动为图片生成描述-Image-Captioning"><a href="#实战项目-1-自动为图片生成描述-Image-Captioning" class="headerlink" title="实战项目 1: 自动为图片生成描述 Image Captioning"></a><a href="#实战项目1-自动为图片生成描述-Image-Captioning" title="实战项目1: 自动为图片生成描述 Image Captioning"></a>实战项目 1: 自动为图片生成描述 Image Captioning</h1><blockquote>
<p>环境安装参考：<a href="https://vel.life/Notes-of-Machine-Learning-Part1/#环境安装与使用" target="_blank" rel="noopener">机器学习初等指南 (1)- 环境安装与使用</a></p>
</blockquote>
<h2 id="Task1-构建-VGG16"><a href="#Task1-构建-VGG16" class="headerlink" title="Task1 构建 VGG16"></a><a href="#Task1-构建VGG16" title="Task1 构建VGG16"></a>Task1 构建 VGG16</h2><p>经典的网络有：<code>VGG</code>, <code>ResNet</code>, <code>DenseNet</code>。</p>
<blockquote>
<p><a href="https://blog.csdn.net/loveliuzz/article/details/79135546" target="_blank" rel="noopener">深度学习卷积神经网络——经典网络 VGG-16 网络的搭建与实现 …</a></p>
<blockquote>
<p>VGGNet 探索了<strong>卷积神经网络</strong>的<strong>深度与其性能</strong>之间的关系，通过反复堆叠 3 <em>3 的小型卷积核和 2</em> 2 的<strong>最大池化层</strong>，VGGNet 成功地构筑了 16~19 层深的卷积神经网络。VGGNet 相比之前 state-of-the-art 的网络结构，错误率大幅下降， VGGNet 论文中全部使用了 3 <em>3 的小型卷积核和 2</em> 2 的最大池化核，通过不断加深网络结构来提升性能。</p>
</blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/41423739" target="_blank" rel="noopener">一文读懂 VGG 网络 - 知乎</a></p>
<blockquote>
<p>简单来说，在 VGG 中，使用了 3 个 3x3 卷积核来代替 7x7 卷积核，使用了 2 个 3x3 卷积核来代替 5x5 卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。</p>
</blockquote>
<p><a href="http://www.deanhan.com/2018/07/26/vgg16/" target="_blank" rel="noopener"><strong>VGG16 学习笔记 | 韩鼎の个人网站</strong></a>，<a href="https://github.com/handsomeboy/vgg16" target="_blank" rel="noopener">https://github.com/handsomeboy/vgg16</a></p>
<p><a href="https://www.kaggle.com/keras/vgg16/home" target="_blank" rel="noopener">Kaggle-VGG16</a></p>
</blockquote>
<h3 id="Pre-卷积"><a href="#Pre-卷积" class="headerlink" title="Pre - 卷积"></a><a href="#Pre-卷积" title="Pre-卷积"></a>Pre - 卷积</h3><blockquote>
<p><a href="https://www.leiphone.com/news/201902/D2Mkv61w9IPq9qGh.html" target="_blank" rel="noopener"><strong>万字长文带你看尽深度学习中的各种卷积网络（上篇）</strong></a></p>
<p><a href="https://www.leiphone.com/news/201902/biIqSBpehsaXFwpN.html" target="_blank" rel="noopener">万字长文带你看尽深度学习中的各种卷积网络（下篇）</a></p>
<p><a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215" target="_blank" rel="noopener"><strong>A Comprehensive Introduction to Different Types of Convolutions in Deep Learning</strong></a></p>
<p><a href="https://www.zhihu.com/question/22298352" target="_blank" rel="noopener">如何通俗易懂地解释卷积？</a></p>
<p><a href="https://www.zhihu.com/question/54504471" target="_blank" rel="noopener">如何理解 Graph Convolutional Network（<strong>GCN</strong>）？</a></p>
<p><a href="https://blog.csdn.net/l691899397/article/details/52250190" target="_blank" rel="noopener">深度学习笔记 5：池化层的实现</a> <code>max-pooling</code>和<code>mean-pooling</code></p>
<p><a href="https://zhuanlan.zhihu.com/p/41696749" target="_blank" rel="noopener">卷积层与池化层</a> <strong>卷积向下取整，池化向上取整</strong>。</p>
<blockquote>
<p>例题：（<a href="https://www.nowcoder.com/questionTerminal/b831a67809fa4ba19d8cf9ed98ff6a21" target="_blank" rel="noopener">Here</a>）</p>
<ul>
<li>输入图片大小为 200×200，依次经过一层卷积（<strong>kernel size 5×5，padding 1，stride 2</strong>），pooling（<strong>kernel size 3×3，padding 0，stride 1</strong>），又一层卷积（<strong>kernel size 3×3，padding 1，stride 1</strong>）之后，输出特征图大小为？</li>
</ul>
<p>答案：<strong>97</strong>。</p>
<p>公式：</p>
<p>𝑓=(𝑚𝑎𝑝𝑠𝑖𝑧𝑒−𝑘𝑒𝑟𝑛𝑒𝑙𝑠𝑖𝑧𝑒+2×𝑝𝑎𝑑𝑑𝑖𝑛𝑔)𝑠𝑡𝑟𝑖𝑑𝑒+1f=(mapsize−kernelsize+2×padding)stride+1 f=\cfrac{(map<em>{size}-kernel</em>{size}+2\times padding)}{stride}+1</p>
<blockquote>
<p>卷积层 1：<code>(input_size - kernel_size + 2*padding)/stride + 1</code>=<code>(200-5+2*1)/2+1 $$\longrightarrow$$floor(99.5)</code>=<code>99</code></p>
<p>池化层：<code>(99-3)/1+1 $$\longrightarrow$$ceil(97)</code>=<code>97</code></p>
<p>卷积层 2：<code>(97-3+2*1)/1+1 $$\longrightarrow$$floor(97)</code>=<code>97</code></p>
</blockquote>
</blockquote>
<p><a href="https://www.zhihu.com/question/41037974/answer/150522307" target="_blank" rel="noopener">全连接层的作用是什么？</a></p>
<blockquote>
<p>全连接层（fully connected layers，<strong>FC</strong>）在整个卷积神经网络中起到 “分类器” 的作用。<strong>在实际使用中，全连接层可由卷积操作实现</strong>：对前层是全连接的全连接层可以转化为卷积核为<code>1x1</code>的卷积；而前层是卷积层的全连接层可以转化为卷积核为<code>hxw</code>的全局卷积，h 和 w 分别为前层卷积结果的高和宽。</p>
<blockquote>
<p>以 VGG-16 为例，对 224x224x3 的输入，最后一层卷积可得输出为 7x7x512，如后层是一层含 4096 个神经元的 FC，则可用卷积核为 7x7x512x4096 的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：</p>
<p>“filter size = 7, padding = 0, stride = 1, D_in = 512, D_out = 4096”</p>
<p>经过此卷积操作后可得输出为 1x1x4096。</p>
<p>如需再次叠加一个 2048 的 FC，则可设定参数为 “filter size = 1, padding = 0, stride = 1, D_in = 4096, D_out = 2048” 的卷积层操作。</p>
</blockquote>
<p>目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数 80% 左右），近期一些性能优异的网络模型如 ResNet 和 GoogLeNet 等均用全局平均池化（global average pooling，GAP）取代 FC 来融合学到的深度特征，最后仍用 softmax 等损失函数作为网络目标函数来指导学习过程。需要指出的是，用 GAP 替代 FC 的网络通常有较好的预测性能。</p>
<p>微调（fine tuning）是深度学习领域最常用的迁移学习技术。</p>
</blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/29119239" target="_blank" rel="noopener">CNN 中卷积层的计算细节</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/40050371" target="_blank" rel="noopener">一文读懂卷积神经网络中的 1x1 卷积核</a></p>
<blockquote>
<p>1x1 卷积一般只改变输出通道数（channels），而不改变输出的宽度和高度。</p>
</blockquote>
</blockquote>
<p><a href="/深度学习训练营/v2-a5b5870ab0a4030684c7f5796e3b83a6_b.gif"><img src="https://vel.life/深度学习训练营/v2-a5b5870ab0a4030684c7f5796e3b83a6_b.gif" alt></a></p>
<p><strong>卷积</strong>：一个函数经过<strong>区间镜像翻转</strong>和<strong>平移</strong>后与另一个函数的乘积的积分。</p>
<p>(𝑓∗𝑔)(𝑡)=∫∞−∞𝑓(𝜏)𝑔(𝑡−𝜏)𝑑𝜏(f∗g)(t)=∫−∞∞f(τ)g(t−τ)dτ (f*g)(t)=\int_{-\infty}^{\infty}f(\tau)g(t-\tau)d\tau</p>
<blockquote>
<p>Source: <a href="http://fourier.eng.hmc.edu/e161/lectures/convolution/index.html" target="_blank" rel="noopener">http://fourier.eng.hmc.edu/e161/lectures/convolution/index.html</a></p>
</blockquote>
<p><a href="/深度学习训练营/5c6bb4c6ce26f.png"><img src="https://vel.life/深度学习训练营/5c6bb4c6ce26f.png" alt></a></p>
<blockquote>
<p>在深度学习中，卷积中的过滤函数是<strong>不经过翻转</strong>的。</p>
</blockquote>
<p><strong>深度卷积</strong>：一个函数<code>f</code>经过<strong>平移</strong>后与另一个函数<code>g</code>的乘积的积分。深度卷积即互关联（Cross-correlation）。</p>
<p><strong>过滤函数</strong>：函数 <code>g</code> 称为一个过滤函数。</p>
<blockquote>
<p>执行卷积的目的就是从输入中提取有用的特征。</p>
</blockquote>
<p><strong>真子空间</strong>：一个空间若<code>真包含于</code>另一个空间，那么它就是另一个空间的子空间。</p>
<p><strong>张量空间</strong>：一个空间如果是以张量（多维离散立方体）的形式定义的，就叫张量空间。</p>
<blockquote>
<p>一张图像就是张量空间（矩阵空间）上的一个点，也就是张量。</p>
</blockquote>
<p><strong>窗口</strong>：一个真子空间称为原空间的一个窗口。</p>
<p><strong>通道</strong>（channel）：如果过滤函数<code>g</code>在张量空间中有一个窗口，<strong>在这个窗口外<code>g</code>恒零</strong>，那么称这个窗口叫<code>g</code>的通道。</p>
<p><strong>卷积核</strong>（kernel）：给定张量空间，一个存在通道的过滤函数<code>g</code>，称为张量空间上的一个卷积核。</p>
<blockquote>
<p>卷积核本身就是张量。一般而言，定义卷积核同时就应该明确指定所使用的通道。</p>
<p>在深度学习中，卷积就是元素级别（ element-wise） 的乘法和加法。</p>
<p>对于一张具有单通道（单卷积核）的图像，卷积过程如上图所示，<strong>过滤函数</strong>是一个组成部分为<code>[[0, 1, 2], [2, 2, 0], [0, 1, 2]]</code>的 3 x 3 矩阵，它<strong>滑动穿过</strong>整个输入。在每一个位置，它都执行了元素级别的乘法和加法，而每个滑过的位置都得出一个数字，最终的输出就是一个 3 x 3 矩阵。（注意：在这个示例中，<code>卷积步长=1</code>；<code>填充=0</code>。）</p>
<p>Source: <a href="https://en.wikipedia.org/wiki/Convolution" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Convolution</a></p>
</blockquote>
<p><a href="/深度学习训练营/5c6bb64e34dd7.gif"><img src="https://vel.life/深度学习训练营/5c6bb64e34dd7.gif" alt></a></p>
<p><strong>滑动卷积</strong>：过滤函数不变，卷积核遍历所有相同规模的窗口，上图所示的过程就是滑动卷积。</p>
<blockquote>
<p>如果按照一定步长迭代，并非遍历的话，也可以有类似的讨论。叫做<strong>广义滑动卷积</strong>。</p>
</blockquote>
<p><strong>覆盖卷积</strong>：如果张量上定义若干个卷积核，这些卷积核的通道<strong>构成张量空间的一个<a href="https://www.wikiwand.com/zh-hans/%E8%A6%86%E7%9B%96_(%E6%8B%93%E6%89%91%E5%AD%A6" target="_blank" rel="noopener">覆盖</a>&gt;)</strong>，则这些卷积核可覆盖卷积。</p>
<blockquote>
<p>滑动卷积是覆盖卷积的特例。</p>
<p>插一句，<a href="https://www.zhihu.com/question/26376319" target="_blank" rel="noopener">有限覆盖定理</a>。（虽然没什么关系~）</p>
</blockquote>
<p><strong>卷积映射</strong>（卷积层）：一个张量被若干卷积核覆盖卷积，获得新张量，则称为卷积映射。</p>
<blockquote>
<p>卷积映射函数即<strong>过滤器</strong>（filter）。有时过滤器也指若干卷积核堆积形成的张量。</p>
<p><strong>一个卷积映射就对应一个卷积层</strong>。</p>
<p>通过卷积映射，原张量被嵌入到一个<strong>相对规模更小的张量空间中</strong>。</p>
<p>卷积核可以不一样哦。不一定等效于滑动卷积了。</p>
</blockquote>
<p><strong>卷积核大小</strong>（Kernel size）：卷积核的窗口的大小。</p>
<p><strong>卷积步长</strong>（Stride）：卷积核滑动通过图像的步长。</p>
<p><strong>填充</strong>（Padding）：填充定义如何处理图像的边界。</p>
<ul>
<li>空填充（将输入边界周围的填充设置为 0，<code>padding=&#39;same&#39;</code>）</li>
</ul>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bc30241434.gif?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bc30241434.gif?imageView2/2/w/740" alt></a></p>
<ul>
<li>不填充（映射后张量变小，<code>padding=&#39;valid&#39;</code>）</li>
</ul>
<blockquote>
<p><a href="https://blog.csdn.net/jasonzzj/article/details/53930074" target="_blank" rel="noopener">Tensorflow 中 padding 的两种类型 SAME 和 VALID</a></p>
<p><a href="https://arxiv.org/pdf/1603.07285.pdf" target="_blank" rel="noopener">A guide to convolution arithmetic for deep learning</a> （《深度学习的卷积算法指南》，详细介绍各种卷积核操作）</p>
</blockquote>
<p><strong>多通道卷积</strong>：如果覆盖卷积中任意一个卷积核都构成滑动卷积，那么称为多通道卷积。</p>
<blockquote>
<p>下面是一个 3 个卷积核的滑动卷积（<strong>RGB</strong> 位图）。一个图像可以是多层的，滑动卷积只需要在某一层上滑动。</p>
<p>Source: <a href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1" target="_blank" rel="noopener">https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1</a></p>
</blockquote>
<p><a href="/深度学习训练营/5c6bba459d2e1.gif"><img src="https://vel.life/深度学习训练营/5c6bba459d2e1.gif" alt></a></p>
<blockquote>
<p>之后，这 <strong>3 个通道都合并到一起</strong>（元素级别的加法）组成了一个大小为 <code>3 x 3 x 1</code> 的单通道。<br>这个通道是<strong>输入层</strong>（5 x 5 x 3 矩阵）使用了<strong>过滤器</strong>（<code>3 x 3 x 3</code> 矩阵）后得到的结果。</p>
</blockquote>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bba4ac0bfb.gif?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bba4ac0bfb.gif?imageView2/2/w/740" alt></a></p>
<blockquote>
<p>更广义的如下图：（紫色部分的长度就是通道数目，<strong>全部合并</strong>）</p>
</blockquote>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bba50054a1.png?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bba50054a1.png?imageView2/2/w/740" alt></a></p>
<blockquote>
<p>这其实就是<strong>高维卷积核</strong>（过滤器）。</p>
<p>如果通道不发生合并：（<code>Din=Dout</code>）</p>
</blockquote>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bbc17a3cb9.png?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bbc17a3cb9.png?imageView2/2/w/740" alt></a></p>
<blockquote>
<p>以及推广的高维卷积核，窗口可以在多个维上滑动：（<code>Din&gt;Dout</code>）</p>
</blockquote>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bbc1e7476d.png?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bbc1e7476d.png?imageView2/2/w/740" alt></a></p>
<blockquote>
<p>在执行计算昂贵的 3 x 3 卷积和 5 x 5 卷积前，往往会使用 1 x 1 卷积来减少计算量。</p>
<p><strong>1 x 1 卷积</strong>最初是在 <a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network-in-network 的论文</a>中被提出的，之后在<a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">谷歌的 Inception 论文</a>中被大量使用。</p>
</blockquote>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bbc23df89f.png?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bbc23df89f.png?imageView2/2/w/740" alt></a></p>
<p><strong>转置卷积</strong>：卷积映射的对应的张量膨胀的卷积称为一个转置卷积。</p>
<blockquote>
<p>如果转置卷积映射存在逆映射，则称为逆卷积。</p>
</blockquote>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bc28a260ef.gif?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bc28a260ef.gif?imageView2/2/w/740" alt></a></p>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bc32c58088.gif?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bc32c58088.gif?imageView2/2/w/740" alt></a></p>
<p>转置卷积的原理如下图所示：</p>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bc3949d069.jpeg?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bc3949d069.jpeg?imageView2/2/w/740" alt></a></p>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bc394991e7.png?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bc394991e7.png?imageView2/2/w/740" alt></a></p>
<blockquote>
<p>转置的含义来自上述原理中的矩阵的转置。而逆来自逆映射。</p>
</blockquote>
<p><strong>空洞卷积</strong>（Dilated Convolution）：在卷积核部分之间插入空间让卷积核膨胀。即扩张卷积。</p>
<p><a href="https://static.leiphone.com/uploads/new/images/20190219/5c6bc939c515b.gif?imageView2/2/w/740" target="_blank" rel="noopener"><img src="https://static.leiphone.com/uploads/new/images/20190219/5c6bc939c515b.gif?imageView2/2/w/740" alt></a></p>
<blockquote>
<p>类似海绵。本质上是在不增加额外的计算成本的情况下<strong>增加感受野</strong>。</p>
<p>《使用深度卷积网络和全连接 CRF 做语义图像分割》（Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs，<a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="noopener">https://arxiv.org/abs/1412.7062</a>）</p>
<p>《通过空洞卷积做多规模的上下文聚合》（Multi-scale context aggregation by dilated convolutions，<a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="noopener">https://arxiv.org/abs/1511.07122</a>）</p>
</blockquote>
<p>还有可分离卷积、扁平化卷积、分组卷积等。（详见 <a href="https://www.leiphone.com/news/201902/biIqSBpehsaXFwpN.html" target="_blank" rel="noopener">Here</a>）</p>
<h3 id="VGG-引述"><a href="#VGG-引述" class="headerlink" title="VGG 引述"></a><a href="#VGG引述" title="VGG引述"></a>VGG 引述</h3><p>VGG 是由 Simonyan 和 Zisserman 在文献《<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener"><em>Very Deep Convolutional Networks for Large Scale Image Recognition</em></a>》（<a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">PDF</a>）中提出卷积神经网络模型，其名称来源于作者所在的牛津大学视觉几何组 (<strong>V</strong>isual <strong>G</strong>eometry <strong>G</strong>roup) 的缩写。</p>
<blockquote>
<p>该模型参加 2014 年的 ImageNet 图像分类与定位挑战赛，取得了优异成绩：</p>
<ul>
<li>在分类任务上排名第二，在定位任务上排名第一。</li>
</ul>
</blockquote>
<p><a href="/深度学习训练营/block.png"><img src="https://vel.life/深度学习训练营/block.png" alt></a></p>
<h3 id="VGG-分类"><a href="#VGG-分类" class="headerlink" title="VGG 分类"></a><a href="#VGG分类" title="VGG分类"></a>VGG 分类</h3><p>VGG 中根据<strong>卷积核大小</strong>和<strong>卷积层数目</strong>的不同，可分为<code>A</code>，<code>A-LRN</code>,<code>B</code>,<code>C</code>,<code>D</code>,<code>E</code>共 6 个配置 (ConvNet Configuration)，其中以<code>D</code>,<code>E</code>两种配置较为常用，分别称为<code>VGG16</code>和<code>VGG19</code>。</p>
<p><a href="/深度学习训练营/table.png"><img src="https://vel.life/深度学习训练营/table.png" alt></a></p>
<blockquote>
<p>对 VGG16 进行具体分析发现，<code>VGG16</code>共包含：</p>
<ul>
<li><strong>13 个卷积层</strong>（Convolutional Layer），分别用<code>conv3-XXX</code>表示</li>
<li><strong>3 个全连接层</strong>（Fully connected Layer）, 分别用<code>FC-XXXX</code>表示</li>
<li><strong>5 个池化层</strong>（Pool layer）, 分别用<code>maxpool</code>表示</li>
</ul>
<p>其中，卷积层和全连接层具有权重系数，因此也被称为<code>权重层</code>，总数目为<code>13+3=16</code>，这即是 VGG16 中 16 的来源。(池化层不涉及权重，因此不属于权重层，不被计数)。</p>
</blockquote>
<h4 id="VGG-的优点"><a href="#VGG-的优点" class="headerlink" title="VGG 的优点"></a><a href="#VGG的优点" title="VGG的优点"></a>VGG 的优点</h4><p>VGG16 的突出特点是<strong>简单</strong>，体现在：</p>
<ul>
<li>卷积层均采用相同的卷积核参数：<ul>
<li>卷积层均表示为<code>conv3-XXX</code>，其中<code>conv3</code>说明该卷积层采用的卷积核的尺寸 (kernel size) 是 3，即宽（width）和高（height）均为 3，<code>3*3</code>是<strong>很小的卷积核尺寸</strong>，结合其它参数（步幅<code>stride=1</code>，填充方式<code>padding=same</code>），这样就能够使得每一个卷积层 (张量) 与前一层（张量）保持相同的宽和高。<code>XXX</code>代表卷积层的通道数。</li>
</ul>
</li>
<li>池化层均采用相同的池化核参数池化层的参数均为 2×2，步幅<code>stride=2</code>，max 的池化方式，这样就能够使得<strong>每一个池化层（张量）的宽和高是前一层（张量）的 1/2</strong>。</li>
<li>模型是由若干卷积层和池化层堆叠（stack）的方式构成，比较容易形成较深的网络结构（<strong>在 2014 年，16 层已经被认为很深了</strong>）。</li>
</ul>
<p>综合上述分析，可以概括 VGG 的优点为: <strong>Small filters, Deeper networks</strong></p>
<h4 id="VGG-的缺点"><a href="#VGG-的缺点" class="headerlink" title="VGG 的缺点"></a><a href="#VGG的缺点" title="VGG的缺点"></a>VGG 的缺点</h4><p>训练时间过长，调参难度大。</p>
<p>存储容量大，不利于部署。（存储 VGG16 权重值文件的大小为 500 多 MB）</p>
<h3 id="块（Block）"><a href="#块（Block）" class="headerlink" title="块（Block）"></a><a href="#块（Block）" title="块（Block）"></a>块（Block）</h3><p>VGG16 的卷积层和池化层可以划分为不同的块（Block），从前到后依次编号为 Block1~block5。</p>
<ul>
<li>每一个块内包含<strong>若干卷积层</strong>和<strong>一个池化层</strong>。</li>
<li>同一块内，卷积层的<strong>通道（channel）数</strong>是相同的。<ul>
<li><strong><code>block3</code></strong>: 3 个卷积层，卷积核尺寸为<code>3*3</code>，通道数都是<code>256</code></li>
</ul>
</li>
</ul>
<p><a href="/深度学习训练营/VGG16.png"><img src="https://vel.life/深度学习训练营/VGG16.png" alt></a></p>
<p>随着层数的增加：</p>
<ul>
<li><strong>卷积通道数翻倍</strong>：64→128→256→512（到 512 不再增加了）</li>
<li><strong>张量尺寸减半</strong>：224→ 112→ 56→28→ 14→ 7（池化层）</li>
</ul>
<h3 id="VGG-参数"><a href="#VGG-参数" class="headerlink" title="VGG 参数"></a><a href="#VGG参数" title="VGG参数"></a>VGG 参数</h3><p>VGG 参数包括<strong>卷积核参数</strong>和<strong>全连接层参数</strong>。<strong>两者都需要学习得到</strong>。</p>
<ul>
<li><strong>卷积核参数</strong><ul>
<li>对于第一层卷积，由于输入图的通道数是 3（<strong>RGB</strong>），网络必须学习大小为 3×3，通道数为 3 的的卷积核，这样的卷积核有 64 个，因此总共有（3×3 × 3）× 64 = 1728 个参数。</li>
</ul>
</li>
<li><strong>全连接层参数</strong><ul>
<li>=<code>前一层节点数</code>×<code>本层的节点数</code>。</li>
</ul>
</li>
<li>最大池化层没有参数</li>
</ul>
<blockquote>
<p>FeiFei Li 在 <a href="https://www.bilibili.com/video/av13260183" target="_blank" rel="noopener">CS231</a> 的课件中给出了整个网络的全部参数（<strong>138 357 544</strong> 个参数）的计算过程（<em>不考虑偏置</em>），如下图所示，图中<strong>红色</strong>是计算所需存储容量的部分；<strong>蓝色</strong>是计算权重参数数量的部分：（<a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf" target="_blank" rel="noopener">Lecture 9: CNN Architectures</a>）</p>
<p><a href="http://deanhan.com/2018/07/26/vgg16/ppt.png" target="_blank" rel="noopener"><img src="http://deanhan.com/2018/07/26/vgg16/ppt.png" alt></a></p>
</blockquote>
<h3 id="VGG16-构建代码"><a href="#VGG16-构建代码" class="headerlink" title="VGG16 构建代码"></a><a href="#VGG16构建代码" title="VGG16构建代码"></a>VGG16 构建代码</h3><table><tbody><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47

</pre></td><td class="code"><pre>from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D

def generate_vgg16():
input_shape = (224, 224, 3) # 输入: 224*244，RGB 三位图
model = Sequential([
Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'), # 卷积层，64 个滤波器（卷积核），尺寸 3*3，参数：输入规格，填充，激活函数
Conv2D(64, (3, 3), padding='same', activation='relu'), # 非首层无需指定输入规格
MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),

        # Block 2
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),

        # 3
        Conv2D(256, (3, 3), padding='same', activation='relu'),
        Conv2D(256, (3, 3), padding='same', activation='relu'),
        Conv2D(256, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
        # 4
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
        # 5
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
        # 全连接层
        Flatten(),
        Dense(4096, activation='relu'),
        Dense(4096, activation='relu'),
        Dense(1000, activation='softmax')
        # 最后要做一个softmax，输出概率归一化
        ])
    return model

if **name** == '**main**': # 主函数，调用
model = generate_vgg16()
model.summary()

</pre></td></tr></tbody></table>

<h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a><a href="#输出" title="输出"></a>输出</h4><table><tbody><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51

</pre></td><td class="code"><pre>Using TensorFlow backend.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 112, 112, 128)     73856     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 28, 28, 512)       1180160   
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 25088)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 4096)              102764544 
_________________________________________________________________
dense_2 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              4097000   
=================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0

</pre></td></tr></tbody></table>

<p>可以看到参数是<code>138,357,544</code>个，基本可以认为构建无误。</p>
<h3 id="VGG19-构建代码"><a href="#VGG19-构建代码" class="headerlink" title="VGG19 构建代码"></a><a href="#VGG19构建代码" title="VGG19构建代码"></a>VGG19 构建代码</h3><p>不妨再模拟一个常用的<code>VGG19</code>。</p>
<p><code>VGG19</code>只是在第 3、4、5 块（Block）各增加一个卷积层。</p>
<table><tbody><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50

</pre></td><td class="code"><pre>from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D

def generate_vgg16():
input_shape = (224, 224, 3) # 输入: 224*244，RGB 三位图
model = Sequential([
Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'), # 卷积层，64 个滤波器（卷积核），尺寸 3*3，参数：输入规格，填充，激活函数
Conv2D(64, (3, 3), padding='same', activation='relu'), # 非首层无需指定输入规格
MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),

        # 2
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),

        # 3
        Conv2D(256, (3, 3), padding='same', activation='relu'),
        Conv2D(256, (3, 3), padding='same', activation='relu'),
        Conv2D(256, (3, 3), padding='same', activation='relu'),
        Conv2D(256, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
        # 4
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
        # 5
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        Conv2D(512, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
        # 全连接层
        Flatten(),
        Dense(4096, activation='relu'),
        Dense(4096, activation='relu'),
        Dense(1000, activation='softmax')
        # 最后要做一个softmax，输出概率归一化
        ])
    return model

if **name** == '**main**': # 主函数，调用
model = generate_vgg16()
model.summary()

</pre></td></tr></tbody></table>

<h4 id="输出-1"><a href="#输出-1" class="headerlink" title="输出"></a><a href="#输出-1" title="输出"></a>输出</h4><table><tbody><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57

</pre></td><td class="code"><pre>Using TensorFlow backend.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 112, 112, 128)     73856     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 56, 56, 256)       590080    
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 28, 28, 512)       1180160   
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 28, 28, 512)       2359808   
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 28, 28, 512)       2359808   
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 14, 14, 512)       2359808   
_________________________________________________________________
conv2d_15 (Conv2D)           (None, 14, 14, 512)       2359808   
_________________________________________________________________
conv2d_16 (Conv2D)           (None, 14, 14, 512)       2359808   
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 25088)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 4096)              102764544 
_________________________________________________________________
dense_2 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              4097000   
=================================================================
Total params: 143,667,240
Trainable params: 143,667,240
Non-trainable params: 0

</pre></td></tr></tbody></table>

<h3 id="QA"><a href="#QA" class="headerlink" title="QA"></a><a href="#QA" title="QA"></a>QA</h3><ol>
<li><p>提交作业时，所有文件都需要上传吗？还是只需要上传修改后的. py 文件？</p>
<p>未回答。（<strong>Remain</strong>）</p>
<p>是的。</p>
</li>
<li><p>代码中卷积核只指定了窗口的大小 3*3，那么卷积核的过滤函数是怎么确定的呢？</p>
<p>~参数通过通过反向传播训练的~。Check！</p>
</li>
<li><p>图像是 RGB 三位（层）的，为什么不是指定 3 个卷积核形成过滤器（滑动窗口），而是 64、128……？步长是 keras 自行确定的吗？指定的卷积核个数是在图像上默认均匀分布吗？而且 64 个 3*3 窗口根本没法覆盖 224x224 的图像啊。。orz</p>
<p>64 代表卷积核的个数，步长自己设，keras 默认步长应该是 1。卷积通过滑动覆盖整个图片。</p>
<p>RGB 只有 3 个通道，64 个卷积核不应该对应 64 个通道吗，怎么滑动？</p>
<p>64 个卷积核是指单个图像被 64 个过滤器<strong>分别过滤</strong>了。64 通道是处理后的通道，224 <em>224</em> 3 <strong>处理</strong>后可以变成 224 <em>224</em> 64。</p>
<p>“处理” 具体对应的是哪一步？什么操作？</p>
<p>处理代表卷积层；通过卷积层之后通道数就变了。</p>
</li>
<li><p>Conv2D 的 filters 和 strides 参数不会互相冲突吗？它们指定的对象有什么区别？</p>
<p>~filters 代表卷积核个数，stride 代表步长，没有冲突~。Check！</p>
</li>
<li><p>多个卷积层堆积到一起究竟有多大意义？我看 padding 都是从输入图像最边缘的一层像素开始的，就算迭代 3 次，外部的 padding 也只能向内传播 3 个像素深度，除了增加算力以外有什么好处吗？我难道不可以直接设置一个具有新的过滤 [复合函数] 的卷积核来完成这样的工作吗？</p>
<p>多个卷积核，每个卷积核的参数都是不一样的，相当于你说的复合函数。</p>
<p>有什么好处？如果相当于复合函数为什么不直接用复合函数？</p>
<p>~是因为神经网络其实本身就是通过不断训练，梯度下降，达到某一个复杂函数的效果~。（<strong>Remain</strong>）</p>
<p>（深层次）训练效果更好。</p>
</li>
<li><p>我看 VGG 模型的 C 配置里有 conv1-512，能讲讲 1 <em>1 卷积核的作用吗？这里的图像也就三层，为什么要反复用 1</em> 1 卷积呢？类似于一个像素级的激活函数吗？（那么还要 ReLU 干啥呢。。）另外 1 _1 为什么要放在 3_3 卷积核的 [后面]？</p>
<p><strong>1 * 1 卷积核用于改变通道个数</strong>，比如从 12 <em>12</em> 256 变成 12 <em>12</em> 512 就可以用 1 * 1 卷积层。</p>
<p>~那不同配置卷积层数不同不影响通道吗？~Check！</p>
</li>
<li><p>池化层到底干了个啥？为啥每次池化了以后通道数翻倍呢？</p>
<p>池化层用于<a href="https://blog.csdn.net/majinlei121/article/details/46742339" target="_blank" rel="noopener">下采样</a>。通道变多是为了组合不同的特征。</p>
<p>我想问的是通道为什么会变多？（不是问 “为了什么而变多”orrrrz）</p>
<p>因为卷积核个数多了，处理代表卷积层；通过卷积层之后通道数就变了。</p>
<p>~卷积可以改变通道数？？？池化也可以改变通道数？~Check！</p>
<p>我还是不太理解卷积核数大于通道数是如何过滤的？神奇的操作？？</p>
<p>嗯嗯，<strong>一个卷积核就可以把之前的所有通道都进行一遍卷积操作</strong>，输出为 n <em>m</em> 1，x 个卷积层就是 n <em>m</em> x，所以 x 跟之前的层有多少通道没关系。</p>
</li>
</ol>
<h2 id="Task2-特征提取"><a href="#Task2-特征提取" class="headerlink" title="Task2 特征提取"></a><a href="#Task2-特征提取" title="Task2 特征提取"></a>Task2 特征提取</h2><p><a href="/深度学习训练营/1553319814542.png"><img src="https://vel.life/深度学习训练营/1553319814542.png" alt></a></p>
<h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a><a href="#迁移学习" title="迁移学习"></a>迁移学习</h3><p><a href="/深度学习训练营/1553320192995.png"><img src="https://vel.life/深度学习训练营/1553320192995.png" alt></a></p>
<p><a href="/深度学习训练营/1553320223350.png"><img src="https://vel.life/深度学习训练营/1553320223350.png" alt></a></p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a><a href="#代码" title="代码"></a>代码</h3><table><tbody><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116

</pre></td><td class="code"><pre>from keras.models import model_from_json
from PIL import Image as pil_image
from keras import backend as K
import numpy as np
from pickle import dump
from os import listdir
import os
from keras.models import Model
import keras
from tqdm import tqdm

def load_img_as_np_array(path, target_size):
"""从给定文件[加载]图像,[缩放]图像大小为给定 target_size,返回[Keras 支持]的浮点数 numpy 数组.

    # Arguments
        path: 图像文件路径
        target_size: 元组(图像高度, 图像宽度).

    # Returns
        numpy 数组.
    """
    img = pil_image.open(path) # 打开文件
    img = img.resize(target_size,pil_image.NEAREST) # NEARSET 是一种插值方法
    return np.asarray(img, dtype=K.floatx()) #转化为向量

def preprocess_input(x):
"""预处理图像用于网络输入, 将图像由 RGB 格式转为 BGR 格式.
将图像的每一个图像通道减去其均值
均值 BGR 三个通道的均值分别为 103.939, 116.779, 123.68

    # Arguments
        x: numpy 数组, 4维.
        data_format: Data format of the image array.

    # Returns
        Preprocessed Numpy array.
    """
    # 'RGB'-&gt;'BGR', https://www.scivision.co/numpy-image-bgr-to-rgb/
    x = x[..., ::-1]
    mean = [103.939, 116.779, 123.68]

    x[..., 0] -= mean[0]
    x[..., 1] -= mean[1]
    x[..., 2] -= mean[2]

    return x

def load_vgg16_model():
"""从当前目录下面的 vgg16_exported.json 和 vgg16_exported.h5 两个文件中导入 VGG16 网络并返回创建的网络模型
vgg16_exported.json 下载链接：链接: https://pan.baidu.com/s/13WQBRb4sr3umP7xbUCxmCg 提取码: ycb5
vgg16_exported.h5 下载链接: https://pan.baidu.com/s/1yF8wybHuzGoTzwSkqTPzzQ 提取码: ub75
注意上传完成的作业时不要上传这两个文件 # Returns
创建的网络模型 model
"""
json_file = open("vgg16_exported.json","r")
loaded_model_json = json_file.read()
json_file.close()

    model = model_from_json(loaded_model_json)
    model.load_weights("vgg16_exported.h5")

    return model

def extract_features(directory):
"""提取给定文件夹中所有图像的特征, 将提取的特征保存在文件 features.pkl 中,
提取的特征保存在一个 dict 中, key 为文件名(不带.jpg 后缀), value 为特征值[np.array]

    Args:
        directory: 包含jpg文件的文件夹

    Returns:
        None
    """
    model = load_vgg16_model()
    # 去除模型最后一层
    model.layers.pop()
    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)
    print("Extracting...")

    features = dict()
    pbar = tqdm(total=len(listdir(directory)), desc="进度", ncols=100)
    for fn in listdir(directory):
        print("\tRead file:", fn)
        fn_path = directory + '/' + fn

        # 返回长、宽、通道的三维张量
        arr = load_img_as_np_array(fn_path, target_size=(224,224))

        # 改变数组的形态，增加一个维度（批处理）—— 4维
        arr = arr.reshape((1, arr.shape[0], arr.shape[1], arr.shape[2]))
        # 预处理图像为VGG模型的输入
        arr = preprocess_input(arr)
        # 计算特征
        feature = model.predict(arr, verbose=0)

        print("\tprocessed...",end='')
        id = os.path.splitext(fn)[0]
        features[id] = feature
        print("Saved. ", id)
        pbar.update(1)

    print("Complete extracting.")
    return features

if **name** == '**main**': # 提取 Flicker8k 数据集中所有图像的特征，保存在一个文件中, 大约一小时的时间，最后的文件大小为 127M # Flickr8k 数据集的下载链接: https://pan.baidu.com/s/1bQcQAz0pxPix9q9kCoZ1aw 提取码: 6gpd # 下载 zip 文件，解压缩到当前目录的子文件夹 Flicker8k_Dataset， 注意上传完成的作业时不要上传这个数据集文件
directory = './Flicker8k_Dataset'
features = extract_features(directory)
print('提取特征的文件个数：%d' % len(features))
print(keras.backend.image_data_format()) #保存特征到文件
dump(features, open('features.pkl', 'wb'))

</pre></td></tr></tbody></table>

<h2 id="Task3-数据生成"><a href="#Task3-数据生成" class="headerlink" title="Task3 数据生成"></a><a href="#Task3-数据生成" title="Task3 数据生成"></a>Task3 数据生成</h2><p><a href="/深度学习训练营/1553327994360.png"><img src="https://vel.life/深度学习训练营/1553327994360.png" alt></a></p>
<h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a><a href="#代码-1" title="代码"></a>代码</h3><table><tbody><tr><td class="gutter"><pre>1
2

</pre></td><td class="code"><pre></pre></td></tr></tbody></table>

<h2 id="Task4-训练网络"><a href="#Task4-训练网络" class="headerlink" title="Task4 训练网络"></a><a href="#Task4-训练网络" title="Task4 训练网络"></a>Task4 训练网络</h2><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a><a href="#代码-2" title="代码"></a>代码</h3><table><tbody><tr><td class="gutter"><pre>1
2

</pre></td><td class="code"><pre></pre></td></tr></tbody></table>

<h2 id="Task5-模型评估"><a href="#Task5-模型评估" class="headerlink" title="Task5 模型评估"></a><a href="#Task5-模型评估" title="Task5 模型评估"></a>Task5 模型评估</h2><h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a><a href="#代码-3" title="代码"></a>代码</h3><table><tbody><tr><td class="gutter"><pre>1
2

</pre></td><td class="code"><pre></pre></td></tr></tbody></table>

<h1 id="实战项目-2：自动驾驶之交通牌识别"><a href="#实战项目-2：自动驾驶之交通牌识别" class="headerlink" title="实战项目 2：自动驾驶之交通牌识别"></a><a href="#实战项目2：自动驾驶之交通牌识别" title="实战项目2：自动驾驶之交通牌识别"></a>实战项目 2：自动驾驶之交通牌识别</h1><h1 id="实战项目-3：-（开放式）自动驾驶之方向盘操纵"><a href="#实战项目-3：-（开放式）自动驾驶之方向盘操纵" class="headerlink" title="实战项目 3： （开放式）自动驾驶之方向盘操纵"></a><a href="#实战项目3：-（开放式）自动驾驶之方向盘操纵" title="实战项目3： （开放式）自动驾驶之方向盘操纵"></a>实战项目 3： （开放式）自动驾驶之方向盘操纵</h1><hr>
<p>相关文章</p>
<ul>
<li><a href="https://vel.life/深度学习中的正则化/" target="_blank" rel="noopener">深度学习中的正则化</a></li>
</ul>

    </div>

    
    
    
        
      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>AILab-aida</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://ailab-aida.github.io/2019/11/08/深度学习训练营 _ 思维之海/" title="深度学习训练营 _ 思维之海">https://ailab-aida.github.io/2019/11/08/深度学习训练营 _ 思维之海/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/算法/" rel="tag"># 算法</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/11/08/Attention 机制的精要总结，附：中英文机器翻译的实现！/" rel="next" title="Attention 机制的精要总结，附：中英文机器翻译的实现！">
                  <i class="fa fa-chevron-left"></i> Attention 机制的精要总结，附：中英文机器翻译的实现！
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/11/08/面试常问问题复习 (二) /" rel="prev" title="面试常问问题复习 (二)">
                  面试常问问题复习 (二) <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="gitalk-container"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">1.</span> <span class="nav-text">References</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#逻辑回归"><span class="nav-number">2.</span> <span class="nav-text">逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#拟合系数的含义"><span class="nav-number">2.1.</span> <span class="nav-text">拟合系数的含义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#应用案例"><span class="nav-number">2.2.</span> <span class="nav-text">应用案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#示例代码"><span class="nav-number">2.2.1.</span> <span class="nav-text">示例代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简单代码"><span class="nav-number">2.2.2.</span> <span class="nav-text">简单代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络"><span class="nav-number">3.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#鸢尾花分类"><span class="nav-number">3.1.</span> <span class="nav-text">鸢尾花分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一些知识"><span class="nav-number">3.1.1.</span> <span class="nav-text">一些知识</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度神经网络"><span class="nav-number">3.2.</span> <span class="nav-text">深度神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度消亡"><span class="nav-number">3.2.1.</span> <span class="nav-text">梯度消亡</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ReLU-激活"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">ReLU 激活</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#非梯度训练方法"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">非梯度训练方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过拟合"><span class="nav-number">3.2.2.</span> <span class="nav-text">过拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Dropout"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L2正则化"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">L2正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L1正则化"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">L1正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MaxNorm"><span class="nav-number">3.2.2.4.</span> <span class="nav-text">MaxNorm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络系数的初始化"><span class="nav-number">3.2.3.</span> <span class="nav-text">神经网络系数的初始化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实战项目-1-自动为图片生成描述-Image-Captioning"><span class="nav-number">4.</span> <span class="nav-text">实战项目 1: 自动为图片生成描述 Image Captioning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Task1-构建-VGG16"><span class="nav-number">4.1.</span> <span class="nav-text">Task1 构建 VGG16</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Pre-卷积"><span class="nav-number">4.1.1.</span> <span class="nav-text">Pre - 卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG-引述"><span class="nav-number">4.1.2.</span> <span class="nav-text">VGG 引述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG-分类"><span class="nav-number">4.1.3.</span> <span class="nav-text">VGG 分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#VGG-的优点"><span class="nav-number">4.1.3.1.</span> <span class="nav-text">VGG 的优点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VGG-的缺点"><span class="nav-number">4.1.3.2.</span> <span class="nav-text">VGG 的缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#块（Block）"><span class="nav-number">4.1.4.</span> <span class="nav-text">块（Block）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG-参数"><span class="nav-number">4.1.5.</span> <span class="nav-text">VGG 参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG16-构建代码"><span class="nav-number">4.1.6.</span> <span class="nav-text">VGG16 构建代码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#输出"><span class="nav-number">4.1.6.1.</span> <span class="nav-text">输出</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG19-构建代码"><span class="nav-number">4.1.7.</span> <span class="nav-text">VGG19 构建代码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#输出-1"><span class="nav-number">4.1.7.1.</span> <span class="nav-text">输出</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#QA"><span class="nav-number">4.1.8.</span> <span class="nav-text">QA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Task2-特征提取"><span class="nav-number">4.2.</span> <span class="nav-text">Task2 特征提取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#迁移学习"><span class="nav-number">4.2.1.</span> <span class="nav-text">迁移学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码"><span class="nav-number">4.2.2.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Task3-数据生成"><span class="nav-number">4.3.</span> <span class="nav-text">Task3 数据生成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代码-1"><span class="nav-number">4.3.1.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Task4-训练网络"><span class="nav-number">4.4.</span> <span class="nav-text">Task4 训练网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代码-2"><span class="nav-number">4.4.1.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Task5-模型评估"><span class="nav-number">4.5.</span> <span class="nav-text">Task5 模型评估</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代码-3"><span class="nav-number">4.5.1.</span> <span class="nav-text">代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实战项目-2：自动驾驶之交通牌识别"><span class="nav-number">5.</span> <span class="nav-text">实战项目 2：自动驾驶之交通牌识别</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实战项目-3：-（开放式）自动驾驶之方向盘操纵"><span class="nav-number">6.</span> <span class="nav-text">实战项目 3： （开放式）自动驾驶之方向盘操纵</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.png"
      alt="AILab-aida">
  <p class="site-author-name" itemprop="name">AILab-aida</p>
  <div class="site-description" itemprop="description">涉猎的主要编程语言为 深度学习、机器学习、大数据、服务端、移动端、前端、爬虫(go、scala、Java、flutter、Python、react、Vue)等。</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">109</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/qq1074123922" title="GitHub &rarr; https://github.com/qq1074123922" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="/1074123922@qq.com" title="E-Mail &rarr; 1074123922@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AILab-aida</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.0</div>

        












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  <script src="/lib/pjax/pjax.min.js?v=0.2.8"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[pjax], script#page-configurations, #pjax script').forEach(element => {
    var id = element.id || '';
    var src = element.src || '';
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (id !=='') {
      script.id = element.id;
    }
    if (src !== '') {
      script.src = src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  








  <script src="/js/local-search.js?v=7.4.0"></script>













    <div id="pjax">

  

  

  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: 'a6d340a24e0f5044ffc3',
      clientSecret: 'edff6432acd3e21caff2696cc123e15b3ca3461c',
      repo: 'ailab-aida.github.io',
      owner: 'AILab-aida',
      admin: ['ailab'],
      id: '728f8aba62e64e426c287fc14ff14a58',
        language: 'zh-CN',
      
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script>

    </div>
</body>
</html>
